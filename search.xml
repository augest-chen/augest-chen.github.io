<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/06/07/cyjournal-15/"/>
      <url>/2020/06/07/cyjournal-15/</url>
      
        <content type="html"><![CDATA[<p>今日总结：毕业照好看，yinan温柔。巴哥、诗忱姐、亮哥合影愿望达成。哈哈哈哈，其实今天还是挺开心的，和大家拍了合照，看到大家都各自在自己的路途上奋斗，我也不能放松呀。</p><p>首先祝我和yinan长长久久啦，</p><p>其次呢，祝愿大家前程似锦，</p><p>最后祝工大进入双一流啦嘻嘻。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200607 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/06/06/cyjournal-14/"/>
      <url>/2020/06/06/cyjournal-14/</url>
      
        <content type="html"><![CDATA[<p>打印完了paper正文，心情复杂。也许四年的生活中终于要结束了。这里要着重着重着重着重感谢一下温柔可爱的，善解人意的诗忱姐，给我和yinan拍照的机会哦！</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200606 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/06/05/cyjournal-13/"/>
      <url>/2020/06/05/cyjournal-13/</url>
      
        <content type="html"><![CDATA[<p>今天刚刚在最美毕业生活动的稿子里面，干了一件大事。第一次承认了是我主动追求了yinan。实际上，在平时，我一直和yinan争论，到底是谁先追的谁。因为是yinan主动亲我的！所以她总是理亏哈哈哈。但是，在公司实习的时候，莫姐说的话，却让我觉得挺有道理。”男孩子在外面，说说的啦。这么好看的女孩子，看上你啥。’’我仔细一想，算了，不想了，是我追的她哈哈哈哈。</p><p>by the way，今天又因为小事情发脾气了，我反思。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200605 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/06/04/cyjournal-12/"/>
      <url>/2020/06/04/cyjournal-12/</url>
      
        <content type="html"><![CDATA[<p>有点忍不住想吐槽了，100页的paper，实习十个月的一个成果，居然不如一个程序都没有实现的论文。论文的评的等级是在搞笑么。工作量、创新度、逻辑严谨度，十个月比不上人家3天的结果？对学校是有点失望噢！！</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200604 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/06/03/cyjournal-11/"/>
      <url>/2020/06/03/cyjournal-11/</url>
      
        <content type="html"><![CDATA[<p>累死了，现在在考虑去买一个单独台式机做并行计算，我感觉即使mac很流畅，但是吃不住大型的计算。yinan最近压力似乎挺大的，也没敢去打扰她。就悄咪咪的一个人上课，一个人搞计算吧。庆幸的是，在孤单的时候总能有一群志同道合之人，也不失为人生的一种乐趣吧。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200603 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>清华｜数据科学</title>
      <link href="/2020/06/03/edx/"/>
      <url>/2020/06/03/edx/</url>
      
        <content type="html"><![CDATA[<h2 id="数据科学导论-Data-Science-A-New-Way-of-Thinking"><a href="#数据科学导论-Data-Science-A-New-Way-of-Thinking" class="headerlink" title="数据科学导论|Data Science: A New Way of Thinking"></a>数据科学导论|Data Science: A New Way of Thinking</h2><h3 id="第一单元：入门、数据采集"><a href="#第一单元：入门、数据采集" class="headerlink" title="第一单元：入门、数据采集"></a>第一单元：入门、数据采集</h3><p>1.数据是现代社会中的一个高频词汇，那么什么是数据呢，简单来说，数据是一种对事物的描述与记录这个事物可以是看得见摸得着的事物，如汽车，也可以是抽象的过程和概念，如经济发展态势，通常来说，人们对周围事物的理解，是通过一系列属性来刻画，如人的年龄 身高 体重 性别 种族等等。所谓的数据对应的则是这些属性可能的取值，根据类型的不同，可分为连续型 离散型 符号型 文本型等等。其中符号型和文本型数据必须先进行数值化，才能够被计算机所处理，因此 数据也可以看作是客观事物的一种抽象表示，用于描述其性质 状态和相互关系等。有助于我们更好地对外部世界进行认知 理解和分析。另一个非常相关的概念是信息，很多时候人们往往将其与数据混为一谈，相对于原始的数据，信息是一种更高层次的抽象。它依托于数据，但体现的是数据的意义与内涵，用以帮助我们进行判断和决策，例如 一个学生的考试成绩，本身可以看作是物理层面上的数据，而通过与其他同学的成绩、或者该生上学期的成绩进行比，就能够得到在班级中的相对水平，或者成绩波动趋势等逻辑层面的信息。当然 信息自身也往往以数据的形式体现出来，如该同学进步明显，与上学期相比班级排名提高了五名，顾名思义，数据科学是一门和数据打交道的学科。随着数据在近十年中变得无处不在，数据科学已经贯穿于，各个学科领域和人们的日常生活，就连一些传统上和计算相关性不大的学科，也开始在工作中，积极寻求大规模数据分析的支持。正如在上世纪七八十年代，计算机还只是科研单位里的高端设备，只有很少数经过专门训练的人员，才有机会接触到它们，而现在从手环到手机，各种类型的计算设备早已和我们24小时相伴，成为日常生活当中不可或缺的一部分，同时 数据科学并不是一项单一的技术，而是包括了数据的采集 传输 存储 分析和展示等诸多环节的一门系统性学科。打一个比方的话，如同先要挖掘和运输铁矿石，再经过粗加工成生铁，再进一步冶炼成标准化的钢材，最后根据各行业的需求，定制成具有特定用途的器件，因此 数据科学并不是一门独立的学科，而是和统计学 信号采集与处理 数据库系统，高性能计算 计算机网络，乃至社会科学等诸多领域有着千丝万缕的联系，提到数据科学。大家首先想到的可能是，写程序和各种各样看上去很高深的算法。首先 计算机程序是数据科学研究工作最终的实现手段，它与数据科学相辅相成，但在一定程度上又相互独立，换句话说，从事数据科学，并不一定要直接从事大量的程序编写工作，也不一定拘泥于某一种特定的开发环境，在更多的情况下是利用各种分析工具，通过结合领域知识，对数据中隐含的信息和规律进行探究。其次，算法是数据科学的核心组成部分，但是仅仅学会算法还不足以真正地，将它转变为一个强大的可以改造我们生活的工具，我们还需要了解数据从何而来，数据背后的物理含义，如何对数据进行预处理，如何解释数据分析的结果，以及在实际应用中，可能产生的种种社会问题等，任何一环的缺失，都可能造成数据分析工作的失败，甚至导致严重的负面效果。这需要引起每一位有志于从事数据科学工作的同学们的高度重视在数据科学中一个重要的概念是大数据。它可能是近年来最炙手可热的科技词汇之一，围绕这个概念的学术研究，商业模式和产业应用举不胜举，大数据的三个典型特征是数据量大 数据类型多和产生速率高分别对应于Volume Variety和Velocity三个属性，统称为3V。下面简单谈一下它们对数据处理技术带来的不同层面的挑战，数据量的激增，对存储和计算能力带来了前所未有的要求，传统上的单机串行处理模式，已经被大规模并行及分布式计算架构所取代，所需处理的数据量也已经从MB跃升到GB和TB，一些大型互联网企业，需要处理的数据甚至已经达到PB级。数据类型的多样化，则对数据分析算法提出了新的挑战，传统算法通常针对单一数据源进行分析，数据类型也极为有限，事实上大多数经典数据挖掘算法，都假设所处理的数据类型为数值型，但是在大数据时代客户的购买记录 评价打分 健康信息，活动轨迹以及在社交网络中与其他人的关系等，都有可能需要进行有机的融合，以做出精准的用户画像，这些数据中既包含传统意义上的结构化数据，又包含大量非结构化的文本数据。同时还可能有显著的数据缺失以及噪声等，这些都对传统算法带来了严峻的挑战，数据的高产生速率带来的问题，可能主要集中在流数据的处理方面，传统上我们一般假设数据是完备的，只要做一次性的处理。当数据源源不断产生的时候，我们则需要算法能够，及时地发现数据中蕴含的新模式并对已有的模型进行适当的动态调整。在我个人看来，大数据分析与传统数据分析相比，最大的特点和优势在于数据类型的多样性，能够从更多的维度对事物进行描述，将不同领域的数据进行关联分析，产生1+1远大于2的效果，显着提高数据分析的效能。</p><p>导论部分，太过于简单的，不再进行记录</p><p>机构产生数据，少量的</p><p>个人产生数据，大量的</p><p>机器产生数据，巨量的</p><h4 id="传感器设计："><a href="#传感器设计：" class="headerlink" title="传感器设计："></a><strong>传感器设计：</strong></h4><p>绝大部分的数据采集都依赖于传感器等硬件。这个传感器节点由电池来供电，上面包含三轴加速度传感器 带微控制器MCU和电源管理电路的，低功耗蓝牙模块等 通常来说设计一个传感器节点时，我们需要关心硬件成本 尺寸 性能和使用寿命等特性</p><p>传感器是一种测量非电学物理量并将其转换为电信号的器件，它们在物理环境和处理器之间提供了接口。它们可以部署在被测对象的内部或者附近。</p><p>大部分麦克风的工作原理是电容式的，也就是当声音进入麦克风，振动膜产生振动，使得振动膜和基板之间的距离发生变化，从而改变他们之间的电容，然后产生变化的电压信号。</p><p>而微机电麦克风属于电容式的变种，他们使用微机电 即MEMS技术，将压力感应膜直接刻蚀在硅片上，硅片上也通常会整合一些相关电路，比如前置放大器，模拟数字转换器等形成数字麦克风，便于和后续数字电路连接。</p><p>还有很多传感器可以告诉我们位置 角度 加速度等信息，这里面的核心是惯性测量单元，简称IMU 它可以测量速度 方向和引力，它将加速度计 陀螺仪和磁力计，这三种类型的传感器集成在一个封装中形成一个基于MEMS的传感器。</p><p>在IMU中 加速度计用于测量X Y和Z方向的加速度，它通过测量组件内部各个方向上的受力情况来得到结果，陀螺仪基于角动量守恒理论用于检测角度以及角动量，磁力计利用磁场原理用来测量磁场强度和方，它还经常用在数字罗盘上</p><p>温度传感器市面上有很多种，例如热电偶 热敏电阻 温度传感芯片等。</p><p>热敏电阻使用陶瓷或者聚合物为材料。这些材料的电阻具备负温度系数，热敏电阻大多是带两个引脚的小圆片，体积相对比较大，而且它随温度变化的是电阻，还需要加上分压网络 运算放大器等外围电路，才能将温度转换为便于电路处理的电压信号</p><p>温度传感芯片，也称为Smart Temperature Sensor，通过利用硅片上器件的温度特性来进行工作，比如三极管的结电压之差与温度成正比。温度传感芯片有很多优点，例如体积小 准确度高 价格便宜等。</p><p>值得一提的是，这些传感器中许多都有噪声，大多数都需要仔细校准</p><h4 id="信号处理："><a href="#信号处理：" class="headerlink" title="信号处理："></a><strong>信号处理：</strong></h4><p>信号处理模块，它对传感器采集到的数据进行压缩编码等预处理。</p><p>除了少数传感器具备数字输出，很多传感器仅输出模拟信号，有时传感器会输出一个非常微弱的信号，在这种情况下，我们必须将它连接到放大器。</p><p>放大器是使用非常广泛的一种硬件电路，为了减小信号失真，放大器需要具备高增益 宽带宽以及低噪声等特性。</p><p>对于放大后的模拟信号，通常还需要进行滤波处理，滤波器类型：模拟滤波器或数字滤波器。</p><p>模拟滤波器依靠，电阻 电容 晶体管等电子元件组成的物理网络，对模拟信号实现滤波功能；而数字滤波器是对模拟信号进行采样，得到数字信号，然后通过数字运算单元，对输入的数字信号进行运算和处理，从而实现滤波特性。</p><p>模拟滤波器通常更高效而且低功耗，而数字滤波器则相对更加灵活，两种实现方式各有利弊。</p><p>滤波后的信号就可以经过模数转换器来进行转换，也就是将模拟信号转换为数字信号，模数转换器是一种非常重要的电路设计高速而且高精度的模数转换器非常具有挑战性。经过转换的数字信号再经过一些数字接口电路就可以进入到处理器中来进行各种预处理。</p><p>在在无线传感器节点中使用得比较多的是微控制器mcu，常用的mcu有msp430系列，具有超低功耗 16位宽片上含模拟数字转换器ADC数字模拟转换器DAC时钟 存储器RAM ROM FLASH支持I2C接口等，价格很便宜。对于32位宽的 则有ARM架构M系列MCU</p><p>MCU具有可编程扩展功能，对于一些无线传感网络，上层通讯协议及信号处理算法等还处于研究发展阶段，要求处理的代码经常会更新或者说对于一些应用。目前的设计并不会一直固定不变，而需要根据实际需求进行调整，那么对于这些应用 MCU就是最好的选择。</p><p>然而对于数字处理器DSP来说，MCU的处理功能还不够强大，所以对于一些需要处理实时音频或视频信号的多媒体感测应用，以及由于外部环境嘈杂需要在本地对数据进行复杂预处理的应用，还可以使用处理能力更强的DSP来做微处理器。现在常用的DSP型号有TMS320系列。</p><p>FPGA称为现场可编程逻辑阵列，单颗芯片具有一定规模的逻辑门阵列，可以通过编程来实现需要的功能，可以反复擦写</p><p>ASIC是专用集成电路，顾名思义 就是专门为一定应用设计的芯片，一旦设计好就无法修改，但是ASIC可以把很多电路集成到一个芯片上，所以在稳定性以及性能等方面有不少优势</p><p>FPGA和ASIC在本质上，是实现数字硬件电路的两种不同方式</p><p>FPGA和ASIC都可以用来实现微处理器，也可以用来设计其它数字电路系统，FPGA现在可以实现从几百门至上千万门规模的电路，理论上可以实现具备任何复杂功能的微处理器，但是其缺点是价格昂贵。</p><p>FPGA的优点是芯片内部的电路可以不断修改，当应用升级时 很容易改进。所以FPGA所实现的微处理器常用在科研领域，主要为了验证所提出的方案，另外对于要求功能强大而且功能时常要升级变化的应用也经常使用FPGA。</p><p>通过ASIC方式我们也可以设计功能强大的微处理器，设计一颗ASIC的费用通常很高，一般会达到数万美金。但是规模效应明显。</p><p>对于一些无线传感器节点，当处理算法复杂而且固定以及要设计的节点数量会非常巨大时，就可以使用ASIC方式来实现需要的微处理器，此外对于航空航天等领域要求芯片鲁棒性极高而且要求高集成度，就算只需要少量的芯片也会使用ASIC方式来实现。相对来说对于价格不敏感。</p><p>关于处理器 目前很热门的还有图形处理器 GPU，这是专门用来进行图形处理的微处理器，传统的CPU内核数量较少，为通用计算而设计而GPU则具有数百或数千个内核，经过优化，可通过并行方式进行大量计算</p><h4 id="无线传感技术："><a href="#无线传感技术：" class="headerlink" title="无线传感技术："></a><strong>无线传感技术：</strong></h4><p>3G、4G,通信技术最远，但是功耗最大，技术也需要授权。</p><p>Wi-Fi，基于IEEE802.11标准，传输范围广，传输速率快，支持在2.4G和5G两个频段工作，2.4G被微波炉、蓝牙等采用较多，因此干扰较多，5G干扰比较少</p><p>传统蓝牙，可以最多包括7个设备。蓝牙是短程范围无线传输设备，蓝牙功耗更低，但是也没有办法实现纽扣电池供电。即使功耗更低的BLE,虽然功耗低了，但是不支持大型文件的传输。</p><p>ZigBee，基于IEEE802.15.4标准，已广泛使用于无线传感器网络设计</p><p>假设在低功耗蓝牙设备中，无线发射功率放大器的典型功率是10毫瓦，一次发射维持5毫秒，使用电压为1.2V，电池容量为120mAh的纽扣电池来供电，该设备每分钟发送两次报告。请问，理论上该电池可以使用多久？</p><p>1.2V x 120mAh / 10mW / 5ms = 1.2 x 120 x 3600 / 10 / 5=10368000;</p><p>10368000 / 2 / 24 / 60=3600天</p><h4 id="能量管理电路："><a href="#能量管理电路：" class="headerlink" title="能量管理电路："></a><strong>能量管理电路：</strong></h4><p>首先 在许多感测应用中，传感器节点使用电池来供电，而且节点的尺寸需要尽可能的小。这也进一步限制了电池的容量，因此我们也需要重点考虑各种能量管理技术来降低工作能耗。</p><p>1.控制算法，非有效工作，关闭电源或者切换至低功耗状态</p><p>2.分析不同电路对于不同电压的需求，设置多种供电电压，优化并降低每一个部分的功耗</p><p>3.能量管理电路，来完成电压转换功能，常用的线性稳压器LDO 只能产生比输入电压低的输出电压,可以使用开关型升压转换器来得到高于输入电压的输出电压</p><p>4.能量收集，太阳能、风能等环境能量的收集，例如占位传感器、自动传感器、光控开关</p><p><img src="/images/nengyuan.png" alt="alt"></p><p>5.无线充电方式，使用线圈磁场（磁感应、磁共振）、超声波、射频波、电场。</p><p>磁感应要求发射和接收线圈靠得足够近，磁共振则可以在收发线圈的相对位置和角度上，提供较高的自由度。在磁共振无线充电系统中，如何在应用参数变化的情况下，保证较高的系统效率及提高鲁棒性，是关键。</p><h3 id="第二单元：数据可视化与高性能计算"><a href="#第二单元：数据可视化与高性能计算" class="headerlink" title="第二单元：数据可视化与高性能计算"></a><strong>第二单元：数据可视化与高性能计算</strong></h3><h4 id="数据可视化："><a href="#数据可视化：" class="headerlink" title="数据可视化："></a><strong>数据可视化：</strong></h4><p>数据可视化正是一项致力于把抽象的数据或概念转化为适宜人类理解和接受的视觉化的信息的技术。</p><p>粗略定义可视化：即可视化是一种以图像、图表或动画的形式进行有效信息传递的技术。</p><p>简单的可视化案例：</p><p>1.霍乱 voronoi图</p><p>2.拿破仑战争分析：flow map</p><p>3.泰坦尼克号生存率分析：mosaic plot</p><p>4.火车时刻表：折线图、flow chart</p><p>5.二维表，含有空间位置：heat map</p><p>高维数据可视化案例：</p><p>1.人口收入国家地区</p><p><img src="/images/map1.png" alt="alt"></p><p>2.雷达图</p><p><img src="/images/map2.png" alt="alt"></p><p>综合评价看面积，单个看指标</p><p>3.平行坐标</p><p><img src="/images/map3.png" alt="alt"></p><p>各个属性上的分布特征</p><p>在极坐标系中，雷达图等价于平行坐标</p><h4 id="高性能计算："><a href="#高性能计算：" class="headerlink" title="高性能计算："></a><strong>高性能计算：</strong></h4><p>差不多直到本世纪初，数据科学的研究更多关注于算法的精度和收敛性等而非算法的执行效率。追求准确率，而不是计算代价有多少大。</p><p>程序：首先考虑的是逻辑上的正确性，以输出正确的结果为核心评价标准。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">以矩阵链&lt;A1,A2,A3&gt;为例，假设三个矩阵的规模分别为<span class="number">10</span>X100，<span class="number">100</span>X5和<span class="number">5</span>X50。</span><br><span class="line"></span><br><span class="line">①以((A1*A2)*A3)方式划分，乘法执行次数为：<span class="number">10</span>*<span class="number">100</span>*<span class="number">5</span>+<span class="number">10</span>*<span class="number">5</span>*<span class="number">50</span>=<span class="number">5000</span>+<span class="number">2500</span>=<span class="number">7500</span>次</span><br><span class="line"></span><br><span class="line">②以(A1*(A2*A3))方式划分，乘法执行次数为：<span class="number">100</span>*<span class="number">5</span>*<span class="number">50</span>+<span class="number">10</span>*<span class="number">100</span>*<span class="number">50</span>=<span class="number">25000</span>+<span class="number">50000</span>=<span class="number">75000</span>次</span><br><span class="line"></span><br><span class="line">我们可以发现，对于同样的矩阵链&lt;A1,A2,A3&gt;相乘而言，由于采用了不同的划分，乘法次数相差<span class="number">10</span>倍。</span><br></pre></td></tr></table></figure><p> 我们可以发现，对于同样的矩阵链&lt;A1,A2,A3&gt;相乘而言，由于采用了不同的划分，乘法次数相差10倍。</p><p>在这里我们简单提一下算法复杂度的概念，一个是算法的时间复杂度，学过数据结构的同学应该都有印象，不同的排序算法的时间复杂度，可能从N的平方到Nlog(N)不等，它描述的是随着问题规模的增大，计算量是如何增加的，例如 对于两个N乘N矩阵的乘法操作，其标准的时间复杂度为N的三次方显然 矩阵乘法操作的计算量，会随矩阵大小的增加而迅速增加，</p><p><img src="/images/oo.png" alt="alt"></p><p>另一个概念是算法的空间复杂度指的是算法在运行中需要额外开辟的存储空间，对于矩阵乘法来说，其空间复杂度是O(1)，即 不随问题规模增大而增大，因为该算法可以不需要额外的临时存储空间</p><p><img src="/images/oo2.png" alt="alt"></p><p>计算机的运行速度通常以GFLOPS、TFLOPS或PFLOPS来衡量，分别代表每秒十亿次 万亿次和千万亿次浮点数操作</p><p>并行计算效率：</p><p><img src="/images/xl1.png" alt="alt"></p><p><img src="/images/xl2.png" alt="alt"></p><p>此外还需要考虑通信的开销</p><p>冒泡排序的外层循环之间存在直接的依赖性，也就是说任何一次循环都需要建立在上一次循环之后产生的结果之上，不能独立或乱序执行，对于内层循环，相邻位的元素要依次进行比较并根据需要进行交换，也不能独立进行，因此冒泡排序本身并不具有可并行性。</p><p>并行计算是一个很广泛的概念，从底层来说，对于多核处理器可以通过分析指令之间的依赖关系找出程序中一组相互独立的指令并行执行，即便是单核处理器，通过对指令进行分解，利用流水线技术也可以实现多条指令的并行执行。</p><p>在宏观层面并行通常分为两种任务并行和数据并行，所谓任务并行指的是在同样的数据上执行不同的操作而数据并行，指的是在不同的数据上执行相似的操作</p><p>并行计算条件：</p><p>首先，指令i的输入和指令j的输出的交集为空，反之亦然；同时两条指令的输出的交集也必须为空。</p><p>第一个条件确保每条指令，不依赖另一条指令的输出作为输入</p><p>第二个条件，确保两条指令，不会同时对相同的变量进行写操作</p><p>线程和进程：</p><p>给定两个计算核心，假设矩阵中对每一个单元的计算需要访问它上下左右四个邻域的值。考虑到通信开销，左边的分配方式高开销。</p><p><img src="/images/jisuan.png" alt="alt"></p><p>虽然在宏观上有多个任务在执行，但是在任意一个时刻只有一个任务占据处理器，这就是所说的Concurrent 翻译为并发。</p><p>而并行指的是在任意一个时刻都有多个任务在同时执行，因此需要多核处理器的支持。所以说并行一定是并发，但反之则不一定成立。</p><p>从程序设计角度出发，并行计算可以分为两大类，进程的并行和线程的并行</p><p>所谓进程就是程序执行的一个实例，例如在集群环境下，同样的程序可以在不同的计算节点多次运行，形成多个进程，作用于不同的数据之上，各个进程之间还可以根据需要传递消息 交换数据 最有代表性的就是MPI标准，线程则依附于进程而存在 是CPU调度的基本单元，在一个进程执行的过程中，可以根据需要释放出多个线程，来执行可以并行的任务 并在完成任务后进行销毁 典型的代表是OpenMP标准。</p><p>多进程模型适用于分布式计算环境，而多线程模型适用于共享内存环境</p><p>GPU计算：</p><p>CPU多核处理器，GPU众核处理器</p><p>基于英伟达公司的GPU和CUDA的开发环境的并行程序设计，其实一点也不复杂，CUDA C本质上是标准C语言的一个拓展。</p><p>GPU并不能脱离于CPU而独立工作，事实上CPU负责整个程序的调度，输入，输出和串行部分的执而GPU作为协处理器，专职负责并行函数的执行，因此整体上属于异构的计算体系。简单来说，需要在GPU上执行的指令和数据，要先传送到GPU上，即从CPU控制的主内存到GPU中的显存，在GPU完成计算后，再把最终结果传送回主内存中。</p><p>基于C语言的GPU程序开发还是有一定门槛。那我就隆重推出《基于GPU的Matlab秘籍》，在数据科学的研究领域，Matlab R和Python，可能是使用最广泛的开发环境。</p><p>例如 我们所熟悉的FFT快速傅立叶变换函数，本身就支持GPU运算。举个例子，首先用rand函数，生成一个1000x1000的随机矩阵，进行傅立叶变换。</p><p><img src="/images/rand.png" alt="alt"></p><p>注意后面两个关键参数：single表示生成的是单精度浮点数，因为GPU相对来讲比较擅长处理单精度数gpuArray 说明数据是生成在GPU的显存，而不是CPU的主内存，这点要特别注意，因为我们现在有两个独立的存储空间。第二步中我们调用Matlab函数FFT，与平时使用的方法没有任何区别。因为FFT内建了对GPU数据的支持，需要注意的是，输出变量也是默认存储在GPU的显存中</p><p>最后要将输出变量的值，通过gather函数传回主内存，也就是我们通常说的Matlab的工作空间中，然后就可以对其进行正常访问了。</p><p>为什么在大规模问题上GPU的性能表现要更好呢，这主要是因为GPU本身的运算速度，要远高于显存的传输速率，当问题规模比较小时数据传输，占据了相对较多的时间，拖累了整体的运算速度。</p><p>并行性能评估：</p><p>通常人们会认为一个计算机系统的运算速度，由它的处理器的性能决定，因此会投入很多资金升级处理。但事实上制约计算机系统的实际运算速度的是数据的传输速率。</p><p>要特别注意数据传输这个瓶颈的存在，只有尽可能降低数据传输的延迟，才有可能实现理想的性能提升和加速效果。</p><p>人们通常采用加速比这个概念，定义为一个程序的串行版本的运行时间，和它的并行版本的运行时间的比值。显然加速比越高，代表并行程序的执行效率越高。</p><p><img src="/images/jisuanxiaolv.png" alt="alt"></p><p>假设两个并行系统的加速比都是十倍，其中一个系统采用了15个计算核心，而另一个系统采用了20个计算核心，那么又该如何评价呢。所以我们引入计算效率这样一个概念，定义为加速比除以计算核心数，显然在同等加速比条件下，使用较少计算核心的系统，拥有较高的计算效率，而且理论上计算效率的最大值为1，即加速比等于计算核心数。</p><p><img src="/images/jisuanxiaolv2.png" alt="alt"></p><p>例如当P等于75%，加速比的上限为4，当P等于95%时，加速比的上限为20。根据阿姆达尔定理，我们可以看出，一个程序的并行潜力取决于，其中不可并行部分所占的比例，如果这个比例过高，则程序的并行性能会受到极大的制约。</p><p>古斯塔夫森定理，也是用来描述程序的理论加速比，但是基于不同的假设。</p><p>假设并行版本程序的运行时间由a+b来表示，分别代表串行和并行部分的运行时间，相对应的串行版本程序的运行时间，则可由a加上N乘以b来表示，其中N为计算核心的数量</p><p><img src="/images/jisuanxiaolv3.png" alt="alt"></p><p>因此该定理表明，假设程序的串行部分耗时固定而可并行部分的规模。随计算核心数量的增加而增加，程序理论上的加速比可以呈线性增长</p><h3 id="第三单元：数据隐私、知识产权法"><a href="#第三单元：数据隐私、知识产权法" class="headerlink" title="第三单元：数据隐私、知识产权法"></a><strong>第三单元：数据隐私、知识产权法</strong></h3><p>数据隐私：</p><p>数据的隐私问题有不同的表现形式，如对个人数据未经授权的访问，对个人敏感信息的发掘与公开，以及对数据进行超过合理范围的使用等等。</p><p>即使信息是公开匿名的，也可以通过一定的关联规则和挖掘，得到真实的信息，侵犯隐私。典型问题，在美国利用邮政编码、性别和生日信息进行身份定位的问题，具体下CMU的参考文献：</p><p>卡耐基梅隆大学的学术论文： <a href="https://dataprivacylab.org/projects/identifiability/paper1.pdf" target="_blank" rel="noopener">https://dataprivacylab.org/projects/identifiability/paper1.pdf</a></p><p>《福布斯》相关报道：</p><p><a href="https://www.forbes.com/sites/adamtanner/2013/07/22/how-just-a-zip-code-can-tell-a-marketer-exactly-who-you-are/#23c70b98426a" target="_blank" rel="noopener">https://www.forbes.com/sites/adamtanner/2013/07/22/how-just-a-zip-code-can-tell-a-marketer-exactly-who-you-are/#23c70b98426a</a></p><p>关于2006年AOL搜索数据公开事件，请参考以下文献：</p><p>维基百科：<a href="https://en.wikipedia.org/wiki/AOL_search_data_leak" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/AOL_search_data_leak</a></p><p>《纽约时报》：<a href="https://www.nytimes.com/2006/08/09/technology/09aol.html" target="_blank" rel="noopener">https://www.nytimes.com/2006/08/09/technology/09aol.html</a></p><p><a href="https://techcrunch.com/2006/08/06/aol-proudly-releases-massive-amounts-of-user-search-data/" target="_blank" rel="noopener">https://techcrunch.com/2006/08/06/aol-proudly-releases-massive-amounts-of-user-search-data/</a></p><p>关于美国超市和未成年少女的故事（并非严格的科学案例）：</p><p>《福布斯》：<a href="https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/#338573a16668" target="_blank" rel="noopener">https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/#338573a16668</a></p><p>个人隐私保护困境：</p><p>个人信息权：有许多个人信息，不一定是私密的</p><p>隐私权：针对私密性信息，个人不愿意公开披露，且不涉及公共利益的内容</p><p>密切联系，但是有区别的。</p><p>个人信息保护的困境，有这样两个困境，第一个困境是我们通过移动互联网获取的便利，是以牺牲个人隐私为代价来换取的。第二个困境是在大数据时代 对信息的去身份化 去标签化的作用正在弱化。大数据时代面临的是海量的立体的数据即使身份被去身份化以后，经过海量数据的分析、对比、组合，还是能够重新恢复个人的身份信息</p><p>因为面临着牺牲隐私以及“去身份化”的困难，所以在对个人信息进行保护时。法律必须解决这样两个重要的问题。</p><p>第一是法律保护范围的问题，法律首先要确定哪些信息，属于被法律保护的个人信息，是不是所有的商业目的的使用都必须严格禁止，是不是必须做到个人信息，不具备可识别性才可以加以利用；另外针对特殊群体的敏感信息是不是要加以保护，对于儿童 残疾人等弱势群体的信息加以保护，现在已经几乎没有了争议，但是对于其他敏感群体呢？</p><p>第二，法律规范的主体问题以前我们通常认为能够侵犯个人信息的主体主要是政府机构或者企业，因此个人信息规范，主要是针对政府和企业的行为，但是随着移动互联网的普及个人也逐渐成为了数据的控制者，很多个人隐私的泄露都是由于个体的行为。</p><p>下面重点介绍，有关个人信息保护的两部最新的立法和一部司法解释，分别是2017年6月实行的《网络安全法》、《最高人民法院 最高人民检察院关于办理侵犯公民个人信息刑事案件适用法律若干问题解释》和2017年10月1日即将施行的《民法总则》</p><p>2016年11月7日，第12届全国人民代表大会常务委员会，第24次会议通过《网络安全法》，自2017年6月1日起施行《网络安全法》针对个人信息保护，有这样四个重要的规定：</p><p>第一，确立了网络运营者对用户信息的保护制度，对于用户个人信息的收集和使用行为进行规范保护网络用户的个人信息，具体而言：</p><p>第40条规定，网络运营者应当对其收集的用户信息严格保密并建立健全的用户信息保护制度</p><p>第41条，要求网络运营者收集使用个人信息应当遵循合法、正当、必要的原则，公开收集使用规则，明示收集使用信息的目的方式和范围必须要经过收集者的同意，同时，规定了网络运营者不得收集与其提供服务无关的个人信息，不得违反法律行政法规的规定，和双方的约定进行收集使用个人信息，应当依照法律行政法规的规定，与用户的约定处理其保存的个人信息</p><p>这一规定非常重要，它强调了网络运营者收集个人信息的边界，也就是说不得收集与其提供的服务无关的个人信息，比如说 地图导航软件需要用户的地理位置这是功能性的要求可以满足</p><p>第二，明确未经同意提供和出售个人信息违法，法律规定的目标是斩断信息买卖的利益链，未经同意提供出售个人信息违法。</p><p>第42条规定：网络运营者不得泄露，篡改，损毁其收集的个人信息，未经被收集人的同意不得向他人提供个人信息，同时《网络安全法》规定了网络运营者应当采取技术措施和其他必要措施确保其收集的个人信息安全，防止信息泄露、损毁、丢失</p><p>第44条规定：任何个人组织不得窃取或者以其他方式获取个人信息，不得以非法出售或非法向他人提供个人信息</p><p>第三，《网络安全法》规定了发生个人信息泄露时通知制度，针对发生个人信息泄露时如何补救，法律规定了相应的补救措施。</p><p>第42条规定：在发生或者可能发生信息泄露，损毁，丢失的情况时，网络运营者应当立即采取补救措施，按照规定及时告知用户并向有关部门上报</p><p>第43条规定：个人发现网络运营者违法或者违反双方的约定，收集使用个人信息的，有权要求网络运营者删除其个人信息</p><p>《网络安全法》明确了个人信息泄露通知制度，要求一旦发生数据泄露事件，服务商要向政府部门和用户报告，确立个人信息泄露通知制度的重要性，在于此前在我国的立法上，只要求企业向主管部门报告，并没有要求向用户告知</p><p>《网络安全法》的新规定，使得个人信息泄露无法彻底杜绝的情况下，能够通过告知用户，将不利的影响减小到最低限度，同时也可以增强用户，对相关诈骗行为的警惕性。这也倒逼了相关机构，提高网络安全的防范能力，降低个人信息泄露的风险</p><p>第四，强调对侵害公民个人信息的惩处措施，根据《网络安全法》的规定，要对网络诈骗溯源追责，对侵害公民个人信息的网络运营者，实施有效的惩处措施</p><p>第64条规定，网络运营者网络产品或者服务的提供者侵害个人信息的可以处以罚款 没收违法所得，暂停相关业务 停业整顿 关闭网站，吊销相关业务许可证或者吊销营业执照的处罚</p><p>《网络安全法》对哪些网络行为应当受到处罚进行了明确的规范，尤其强调了网络运营者，维护个人信息安全的主体责任，除了罚款和没收违法所得之外，关闭网站 吊销营业执照的威慑力更加大，针对侵害公民个人信息的刑事责任，最高人民法院和最高人民检察院的最新司法解释进行了规定</p><h3 id="第四单元：环境大数据"><a href="#第四单元：环境大数据" class="headerlink" title="第四单元：环境大数据"></a><strong>第四单元：环境大数据</strong></h3><h2 id="数据挖掘：理论与算法-Data-Mining-Theories-and-Algorithms-for-Tackling-Big-Data"><a href="#数据挖掘：理论与算法-Data-Mining-Theories-and-Algorithms-for-Tackling-Big-Data" class="headerlink" title="数据挖掘：理论与算法 | Data Mining: Theories and Algorithms for Tackling Big Data"></a>数据挖掘：理论与算法 | Data Mining: Theories and Algorithms for Tackling Big Data</h2><h3 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a><strong>第一部分</strong></h3><h3 id="走进数据"><a href="#走进数据" class="headerlink" title="走进数据"></a><strong>走进数据</strong></h3><p>这一部分基本是简单的介绍，我就稍微选几个我觉得是重点的吧。需要理解什么是data rich information poor。</p><p><strong>其次，一些教材推荐：</strong></p><p>1.数据挖掘：概念与技术</p><p>2.模式分类，3.beautiful data （这本书案例多，可以拓展思路）4.data mining：practical machine learning tools and techniques 5. introduction to data mining</p><p><strong>以及需要关注一些国际会议的论文：</strong></p><p>international conference on data mining</p><p>international conference on data engineering</p><p>international conference on machine learning</p><p>international joint conference on artificial intelligence</p><p>Pacific-asia conference on knowledge discovery and data mining</p><p>ACM SIGKDD conference on knowledge discovery and data mining</p><p>ieee transaction knowledge and data engineering</p><p>ieee transaction neural newtworks and learning systems</p><p>data mining and konwledge discovery</p><p>information sciences</p><p><strong>两个协会：</strong></p><p>ieee computiatioal intelligence society</p><p>ieee computer society</p><p><strong>几个华人学者的实验室官网，关注其进度：</strong></p><p>xindong wu</p><p>zhihua zhou</p><p>jiawei han</p><p>jian pei</p><p>qiang yang</p><p>chih-jen lin</p><p>philip s. yu</p><p>changshui zhang</p><p><strong>一些工具的推荐：</strong></p><p>1.检索类：</p><p>Wikipedia</p><p>google </p><p>google scholar</p><p>kddnuggest 专门的数据挖掘网站</p><p>2.工具类</p><p>matlab 的 app</p><p>UCI machine learning repository 上的公开数据集</p><p>weka 这是一款开源的数据挖掘软件</p><p>数据挖掘的广义概念如下图：</p><p><img src="/images/datamining.png" alt="alt"></p><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a><strong>数据</strong></h3><p>数据的定义（粗略的）：定量、定型化的属性值。数据是底层的一种表现形式，信息是高一层的。数据处理后的，才能叫做信息。数据不同类型，连续型的(continues)，二值型(binary)的。存储上，数据又可以分为逻辑下那型和物理层面的。物理上都是01的，但是逻辑上，可以是二维表等。在实际数据分析工作中，数据类型转换和数据自身的错误是面临的主要挑战之一。</p><p>big data ： gartner的定义：数据量大，速度快（产生速度），数据种类多（千奇百怪）/mckinsey：超过传统数据处理的能力的数据</p><p>三个特点：</p><p>1.结构化变为非结构化</p><p>2.batch变为数据流</p><p>3.数据变大了</p><p><img src="/images/bigdata.png" alt="alt"></p><p>大数据应用：</p><p>1.公共安全：预防犯罪</p><p>2.医疗健康：undertreat &amp; overtreat、 基因</p><p>3.城市规划：智慧城市</p><p>4.地理位置：定位、商店购物</p><p>5.情感分析：sentiment amalysis</p><p>6.社交网站：</p><p>7.点球成金；</p><h3 id="数据到知识"><a href="#数据到知识" class="headerlink" title="数据到知识"></a><strong>数据到知识</strong></h3><p>开放数据，难以获得，因此，产生了数据孤岛。有一种趋势就是去打破这些数据孤岛。也就是数据公开。公开数据有两个要求：1.技术公开(可公开，易获得)，2.法律公开（开放，公开）。</p><p>以下有可获得公开数据的来源：</p><p><img src="/images/opendataset.png" alt="alt"></p><p>此外，补充一个政府数据公开的网站：</p><p><a href="https://www.data.gov/" target="_blank" rel="noopener">美国政府公开数据网站</a></p><p>数据挖掘：从以前的统计分析，到现代的大型计算机引入的统计方式都是数据挖掘的一部分。<strong>数据挖掘应该是一个过程，一个自动化的过程，一个自动化提取有趣（interesting）、有用（useful）、隐藏（hidden）的模式（pattern），从海量的（massive）、不完整的（incomplete）、嘈杂（noisy）的数据中。</strong>并且，需要注意的是，并不是全自动的过程。数据挖掘，并不是，没有人的涉及的。数据挖掘（data mining），有一个近义词，知识发现（konwledge discovery）</p><p>数据挖掘的流程：</p><p><img src="/images/liuc.png" alt="alt"></p><p>业界的数据挖掘流程：</p><p><img src="/images/qiye.png" alt="alt"></p><p>其中ETL,又可以被叫做数据融合。ETL系统的作用主要是数据提起、数据转换、数据装载</p><p>可以把这些过程进行抽象，得到理论上的过程：</p><p><img src="/images/lilun.png" alt="alt"></p><h3 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a><strong>算法简介</strong></h3><h4 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a><strong>分类问题</strong></h4><p>定义：数据挖掘当中第一类问题就叫做Classification 分类问题，什么叫分类问题 你给我一批人过来，每个人有一些属性标签，身高体重年龄 反正这样的，一些属性 然后你要预测什么，比如他是好人还是坏人 我建一个模型能做这件事情 这就叫做分类。<strong>我为什么能够给他打标签，好人还是坏人 因为我以前已经见过，一批打过标签的人 我已经知道了</strong>，好人长什么样坏人长什么样，所以现在再来一个人 我就可以去，根据以前我学到的这些知识我就，来判断新来的这个人他到底是好，这就叫classification</p><p>典型的算法：决策树、KNN、神经网络、支持向量机</p><p>直观的来看：其实就是分界面：</p><p><img src="/images/classexample.png" alt="alt"></p><p>随着情况的复杂，有的时候一条线可能并不能完整的划分的很好，因此，也可以使用曲面、</p><p><img src="/images/classexample2.png" alt="alt"></p><p>本质上，这就是对于空间进行划分。不同的分类器的结果也是不一样的。重要的是：平滑的分类曲线更加重要！</p><p>下图的绿色的分类器，就是过拟合的现象，而黑色的，更加平滑，即使有一定的错误，但是更加的平稳。</p><p><img src="/images/fenleiqi.png" alt="alt"></p><p>在数据挖掘的时候，一定要注意划分测试集(test set)和训练集(traning set),流程如下：</p><p><img src="/images/liuchengtu.png" alt="alt"></p><p>混淆矩阵：注意准确率的计算方法</p><p><img src="/images/hunxiaojuzhen.png" alt="alt"></p><p>AUC的概念：首先，看到左上角的图片，红色的正态分布为男人的身高，蓝色的正态分布代表女人的身高。来选择一个合适的分类器，来通过身高划分男人和女人。如果这个分类器设置为：这个人身高超过10m，才算是男人，那么没有人会别分类为男人，因此，预测为男，实际为男的数量就是0，然后预测为女，实际为男的数量就是100%。得到第一组坐标，就是[0,100%]，同理，如果分类器设置为：这个人升高超过1m，就是男人。那么可以得到第二组坐标[100%,0]。由此，我们将其在中间的图上画出。</p><p><img src="/images/nanrennvrenwenti.png" alt="alt"></p><p>如果分类的调整是随机的（random process）,那么这两点的连线，就是随机的分类器的曲线。但是，我们希望的是分类器的曲线是上图的曲线的。</p><p><img src="/images/fenleiqi2.png" alt="alt"></p><p>但是不同的分类器，都有着不同的曲线，未来衡量他们的效果。求解曲线到x轴的面积，我们叫做auc，使用其作为衡量分类器效果的一个标准。AUC上限为1，随机的AUC为0.5。这一过程，也叫做ROC分析</p><p><strong>cost sensitive learning：</strong></p><p>实际过程中，不同错误的危害是不一样的。实际操作，记得考虑成本</p><p><strong>lift analysis：</strong></p><p>用了模型和不使用模型的差异就是提升度(lift)，这个可以说是另外一种分析的模型效果的方法。</p><p>简单例题：假设目标客户占人群的5%，现根据用户模型进行打分排序，取1000名潜在客户中排名前10%的客户，发现其中包含25名目标客户，问此模型在10%处的提升度是多少？</p><p>1000*5% = 50人</p><p>目标客户有50人</p><p>1000*10% = 100人</p><p>取得100人做预测</p><p>25/100 = 25%</p><p>模型的效率25%</p><p>25%/5% = 5</p><p>提升度为5</p><h4 id="聚类问题"><a href="#聚类问题" class="headerlink" title="聚类问题"></a><strong>聚类问题</strong></h4><p>定义: 聚类 什么叫聚类，同样你给我一堆items，然后把它分分group 大家注意要把它，分成一组一组的 这个东西叫聚类。注意聚类是无监督学习，但是分类是监督学习。监督体现在，分类问题中，标签是预先知道的。</p><p>聚类的距离：曼哈顿、欧式距离等，算法：K-means、层次聚类等</p><h4 id="关联度"><a href="#关联度" class="headerlink" title="关联度"></a><strong>关联度</strong></h4><p>牛奶面包案例</p><h4 id="回归模型"><a href="#回归模型" class="headerlink" title="回归模型"></a><strong>回归模型</strong></h4><p>这里先略过，后续有具体的。下图，全部为线型回归，所谓线性回归，指的是beta这个系数和x是线性，也就是beta*x是线性，而不是整体是不是线性。</p><p><img src="/images/reg.png" alt="alt"></p><p>回归也是存在过拟合的问题。要考虑平滑度。</p><h4 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a><strong>可视化</strong></h4><p><img src="/images/vs.png" alt="alt"></p><h4 id="数据质量"><a href="#数据质量" class="headerlink" title="数据质量"></a><strong>数据质量</strong></h4><p>做数据挖掘，最重要的是要进行数据预处理。必须重视数据的预测护理。不然容易出现gigo问题。也就是垃圾进，垃圾出。</p><p><img src="/images/qs.png" alt="alt"></p><p>数据预处理的步骤：</p><p><img src="/images/clear.png" alt="alt"></p><h4 id="数据隐私"><a href="#数据隐私" class="headerlink" title="数据隐私"></a><strong>数据隐私</strong></h4><p>因为隐私问题，想要实现：收集数据的时候，并不知道每一个填问卷的人的具体答案，但是当所有人的问卷收集上来的时候，可以知道全体人员中选择这一答案的问题，是比较难的。</p><p><img src="/images/yinsi.png" alt="alt"></p><p>将同一个问题表述为两个问题。然后分为AB卷。最后在统计结果。</p><p>典型过程如下：</p><p><img src="/images/wenjuan.png" alt="alt"></p><h4 id="云计算"><a href="#云计算" class="headerlink" title="云计算"></a><strong>云计算</strong></h4><p>本意：服务器的需求有峰谷，按道理必须要满足顶峰时候的服务器需求，但是这就造成了谷峰时候的服务器资源的闲置。出于优化配置的考虑，将买服务器将变为组服务器（pay as you go）</p><p>software as a service（SAAS）租软件等</p><p>platform as a service 租开发环境等</p><p>Infrastructure as a service（IAAS）租cpu、内存、硬盘等</p><h4 id="并行计算"><a href="#并行计算" class="headerlink" title="并行计算"></a><strong>并行计算</strong></h4><p>愚公移山是串行，并行计算是并行。推荐使用GPU,但是cpu也可以。但是目前GPU还是需要和CPU协同工作，构成异构计算平台。</p><h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a><strong>summary</strong></h4><p>数据+算法+硬件支撑 = 数据挖掘</p><p>一切要取决于你的需求，例如客户需要解释，那就搞决策树，那是可解释的，如果不需要解释的，也许可以试一下神经网络。拿到问题，先由简单，在试探复杂的。</p><p><img src="/images/summm.png" alt="alt"></p><h4 id="cases"><a href="#cases" class="headerlink" title="cases"></a><strong>cases</strong></h4><p>量化交易：</p><p><img src="/images/nas.png" alt="alt"></p><p>可以尝试</p><p>彩票：</p><p><img src="/images/caipiao.png" alt="alt"></p><p>不能挖掘的问题在于，底层逻辑就是随机的。数据挖掘本质就是挖掘规律，不是创造规律</p><p>内在分组：</p><p><img src="/images/neizai.png" alt="alt"></p><p>整体负相关，但是分组后，又是正相关。因此，要重点注意数据内部分组。</p><p>幸存者存在偏差：</p><p>二战的轰炸机的案例</p><p>无内在联系：</p><p>我的工资增长和美国的青少年的犯罪率有着负相关？不合理。</p><p>时间维度：</p><p><img src="/images/shijian.png" alt="alt"></p><p>到底是商家店铺数量减少，营业额减少。还是数量增加，营业额增加。</p><h3 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a><strong>第二部分</strong></h3>]]></content>
      
      
      <categories>
          
          <category> edx </category>
          
          <category> thu data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> edx </tag>
            
            <tag> thu </tag>
            
            <tag> data science </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/06/02/cyjournal-10/"/>
      <url>/2020/06/02/cyjournal-10/</url>
      
        <content type="html"><![CDATA[<p>作为杭州人，61过儿童节，62过62节哈哈哈哈。不过呢，今天yinan似乎挺烦躁的。点个烧烤，给他安慰一下内心吧。不过到时候又要怪我，给他点夜宵，减不了肥了。哈哈哈，今天也没有什么大事，定个小目标，在利用接下来的两个月刷完coursera和edx的课程。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200602 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/06/01/cyjournal-9/"/>
      <url>/2020/06/01/cyjournal-9/</url>
      
        <content type="html"><![CDATA[<p>六一快乐，yinan。在学校的日子还是比较清闲的。感觉需要找点事情来做。明天就找yinan一起学习。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200601 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>coursera 1st the global financial crisis</title>
      <link href="/2020/06/01/coursera-4/"/>
      <url>/2020/06/01/coursera-4/</url>
      
        <content type="html"><![CDATA[<p>A number of students have asked for the name of a textbook so that they might read more on the content covered in the course. There is no one textbook that covers the breadth of the course. Even when we teach the course at Yale, we use a compilation of various articles and texts. Below are some resources that will enable you to explore further the key themes addressed in the course. Also see the Suggested Readings posted with each weekly module.</p><p>Thank you for your continued participation in the Global Financial Crisis. We hope you enjoy the course!</p><p>Yale Coursera Team</p><p><strong>Recommended Additional Readings week 1</strong></p><p><strong>P1</strong></p><p>\1. Geithner, Timothy, Stress Test, Introduction + Chapters One and Two</p><p>\2. Bernanke, Ben, 2010, <a href="http://som.yale.edu/sites/default/files/files/Bernanke_Ben_Causes_of_the_Recent_Financial%26Economic_Crisis_9-2-10.pdf" target="_blank" rel="noopener">Testimony to the Financial Crisis Inquiry Commission.</a></p><p>\3. Gorton, Gary B. and Andrew Metrick, 2012, <a href="http://faculty.som.yale.edu/garygorton/documents/GettingUpToSpeed_Jan-11-2012.pdf" target="_blank" rel="noopener">Getting up to speed on the financial crisis: a one-weekend-reader’s guide</a>, <em>Journal of Economic Literature</em> 2012, 50:1, 128–150</p><p>\4. Schularick, Moritz and Alan M. Taylor 2012, <a href="http://www.aeaweb.org/articles?id=10.1257/aer.102.2.1029" target="_blank" rel="noopener">Credit booms gone bust: monetary policy, leverage cycles and financial crises, 1870-2008</a>, <em>American Economic Review</em> 102, 1029-1061.</p><p><strong>P2</strong></p><p>\1. Bernanke, Ben S.,., The Recent Financial Turmoil and its Economic and Policy Consequences. Speech at the Economic Club of New York, New York, New York. 2007.</p><p><em>— The former Chairman of the Federal Reserve discusses events occurring at the early stages of the crisis and the spillover from the subprime crisis to the financial system.</em></p><p>\2. Bernanke, Ben S., Testimony to the Financial Crisis Inquiry Commission, 2010.</p><p><em>— The former Chairman of the Federal Reserve discusses some of the elements that lead up to the crisis including increased leverage and the reliance on short-term funding by financial institutions. He also discusses some of the weaknesses in the regulatory system and the problem of too-big-to-fail financial institutions.</em></p><p>\3. Financial Crisis Inquiry Commission. The Financial Crisis Inquiry Report: Final Report of the National Commission on the Causes of the Financial and Economic Crisis in the United States, authorized ed. New York: Public Affairs, 2011.</p><p>— <em>The FCIC was established to examine the causes of the financial and economic crisis in the United States. The final report presents the commission’s findings and conclusions and also includes dissenting views. A Table of Contents makes it easy to locate particular topics. The FCIC website also has searchable original documents and testimony as well as various reports prepared by commission staff.</em></p><p>\4. Geithner, Timothy F., Stress Test. New York, Crown Publishing Group, 2014.</p><p><em>—Former President of the Federal Reserve Bank of New York and Secretary of the Treasury recounts the financial crisis from a first person perspective. The book presents unique insights as to the pressures, perceptions and timing under which decisions were made.</em></p><p>\5. Gorton, Gary B. and Andrew Metrick. Getting up to Speed on the Financial Crisis: A One-Weekend-Reader’s Guide, Journal of Economic Literature 2012, 50:1, 128-150. 2012.</p><p><em>—The authors discuss 16 selected documents, including academic papers and reports from regulatory and international agencies that provide a picture of the factors leading up to the financial crisis and the actions taken by governments in response. Articles discuss the global savings glut, panic in short-term debt, contagion and effects on the real economy.</em></p><p>\6. Sorkin, Andrew Ross. Too Big to Fail: The Inside Story of How Wall Street and Washington Fought to Save the Financial System—and Themselves. New York: Viking Press, 2009.</p><p><em>—The author presents a very readable and journalistic account of March-October 2008, the most critical period of the financial crisis, based on extensive interviews with key participants from industry and government.</em></p><p><em>7. Wessel, David. In Fed We Trust: Ben Bernanke’s War on The Great Panic. New York: Three Rivers P</em>ress, 2009.</p><p><em>—Focusing on the Federal Reserve’s unprecedented role, this book presents a highly-accessible analysis of the key players, critical decisions, underlying policy, and market turbulence that characterized the financial crisis.</em></p><p><strong>Recommended Additional Readings week 2</strong></p><p><strong>P1</strong></p><p>\1. Geithner, Timothy, Stress Test, Chapter Three</p><p>\2. Jobst, Andreas, 2008, <a href="http://http//www.imf.org/external/pubs/ft/fandd/2008/09/basics.htm" target="_blank" rel="noopener">Back to basics-what is securitization?</a> Finance &amp; Development 45, 48.</p><p>\3. Pozsar, Zoltan. 2011, <a href="http://https//www.imf.org/external/pubs/cat/longres.aspx?sk=25155.0" target="_blank" rel="noopener">Institutional Cash Pools and the Triffin Dilemma of the U.S. Banking System</a>, International Monetary Fund Working Paper 11/190.</p><p>\4. Bernanke, Ben S., Carol Bertaut, Laurie P. DeMarco, and Steven Kamin, 2011, <a href="http://https//www.federalreserve.gov/pubs/ifdp/2011/1014/ifdp1014.htm" target="_blank" rel="noopener">International capital flows and the return to safe assets in the United States, 2003-2007</a>, FRB International Finance Discussion Paper No. 1014.</p><p>\5. Gorton, Gary B. and Andrew Metrick, <a href="http://http//www.sciencedirect.com/science/article/pii/B978044453594800001X" target="_blank" rel="noopener">Securitization, 2013, Handbook of the Economics of Finance, Volume 2A</a>, George Constantinides, Milton Harris, and Rene Stulz eds., 1-70, Elsevier. (Just read the first three sections).</p><p><strong>Recommended Additional Readings week 3</strong></p><p><strong>P1</strong></p><p>\1. Bernanke, Ben S., <a href="https://www.federalreserve.gov/newsevents/speech/bernanke20070517a.htm" target="_blank" rel="noopener">The Subprime Mortgage Market</a>, Speech at the Federal Reserve Bank of Chicago’s 43rd Annual Conference on Bank Structure and Competition, Chicago, Illinois, May 17, 2007.</p><p>\2. Shiller, Robert J., 2007, <a href="http://www.kansascityfed.org/publicat/sympos/2007/PDF/Shiller_0415.pdf" target="_blank" rel="noopener">Understanding Recent Trends in House Prices and Homeownership</a>, in <em>Proceedings</em> of the symposium “Housing, Housing Finance, and Monetary Policy.” Kansas City: Federal Reserve Bank of Kansas City, pp.89-123.</p><p>\3. Mayer, Christopher, Karen Pence, and Shane M Sherlund, 2009, <a href="http://www.federalreserve.gov/pubs/feds/2008/200859/200859pap.pdf" target="_blank" rel="noopener">The Rise in Mortgage Defaults</a>, <em>The Journal of Economic Perspectives</em> 23, 27-50.</p><p>\4. Foote, Christopher L, Kristopher S Gerardi, and Paul S Willen, 2012, <a href="http://www.nber.org/papers/w18082" target="_blank" rel="noopener">Why did so many people make so many ex post bad decisions? The causes of the foreclosure crisis</a>, working paper, National Bureau of Economic Research.</p>]]></content>
      
      
      <categories>
          
          <category> coursera </category>
          
          <category> the global financial crisis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> coursera </tag>
            
            <tag> the global financial crisis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云计算概论</title>
      <link href="/2020/05/31/azure/"/>
      <url>/2020/05/31/azure/</url>
      
        <content type="html"><![CDATA[<p>云计算会租用其他公司计算机上的存储空间或 CPU 周期等资源。 只需为所用的部分付费。 提供这些服务的公司称为云提供商。 某些提供商包括 Microsoft、Amazon 和 Google。</p><p>云提供商负责提供执行工作所需的物理硬件，并使其保持最新。 提供的计算服务往往因云提供商而异。 但是，它们通常包括：</p><ul><li><strong>计算能力</strong> - 例如，Linux 服务器或 Web 应用程序</li><li><strong>存储</strong> - 例如，文件和数据库</li><li><strong>网络</strong> - 例如，云提供商和公司之间的安全连接</li><li><strong>分析</strong>例如，可视化遥测和性能数据</li></ul><h2 id="云计算服务"><a href="#云计算服务" class="headerlink" title="云计算服务"></a>云计算服务</h2><p>云计算的目标是使企业开展业务更轻松、更高效，无论是小型初创企业还是大型企业。 每项业务都是独特的，有不同的需求。 为满足这些需求，云计算提供商提供广泛的服务。</p><p>需要对其提供的一些服务有基本了解。 我们简要讨论所有云服务提供商都提供的两种最常见服务：–<em>计算能力<em>和</em>存储</em>。</p><h3 id="计算能力"><a href="#计算能力" class="headerlink" title="计算能力"></a>计算能力</h3><p>发送一封电子邮件、在 Internet 上进行预订、在线付款、或甚至使用此 Microsoft Learn 模块时，你都在与处理每个请求并返回响应的基于云的服务器进行交互。 作为消费者，我们都依赖于组成 Internet 的各种云提供商提供的计算服务。</p><p>使用云计算生成解决方案时，可以根据自己的资源和需求选择工作的完成方式。 例如，如果希望对维护拥有更多控制权并承担更多责任，则可以创建虚拟机 (VM)。 VM 是仿真计算机，与你正在使用的台式机或笔记本电脑相似。 每个 VM 都包括操作系统和硬件，在用户看来就像运行 Windows 或 Linux 的物理计算机一样。 然后，你可以安装任何所需的软件来执行要在云中运行的任务。</p><p>不同之处在于，你无需购买任何硬件或安装操作系统。 云提供商在其中一个数据中心的物理服务器上运行虚拟机，通常与其他 VM 共享该服务器（独立且安全）。 借助云，你能够以比物理计算机更低的成本在数分钟内准备好 VM。</p><p>VM 不是唯一的计算选择，还有两个其他常用选项：容器和无服务器计算。</p><h4 id="什么是容器？"><a href="#什么是容器？" class="headerlink" title="什么是容器？"></a>什么是容器？</h4><p>容器为应用程序提供一致、独立的执行环境。 它们类似于 VM，但它们不需要来宾操作系统。 相反，应用程序及其所有依赖项都打包到“容器”中，然后使用标准运行时环境来执行应用。 这样，容器可以在数秒钟内启动，因为没有要启动和初始化的操作系统。 只需启动应用。</p><p>开放源代码项目 Docker 是用于管理容器的领先平台之一。 Docker 容器为应用程序部署提供了一种高效、轻量级的方法，因为通过它们可将应用程序的不同组件独立部署到不同的容器中。 多个容器可以在一台计算机上运行，并且容器可以在计算机之间移动。 由于容器的可移植性，可以非常轻松地将应用程序部署到多个环境（无论是在本地还是在云中），通常无需对应用程序进行任何更改。</p><h4 id="什么是无服务器计算？"><a href="#什么是无服务器计算？" class="headerlink" title="什么是无服务器计算？"></a>什么是无服务器计算？</h4><p>借助无服务器计算，无需创建、配置或维护服务器即可运行应用程序代码。 核心理念是将应用程序分为单独的函数，这些函数会在由某些操作触发时运行。 这是自动化任务的理想之选。例如，你可以构建无服务器进程，在客户进行在线购买后自动发送电子邮件确认。</p><p>无服务器模型与 VM 和容器的不同之处在于，你只需为每个函数在执行时使用的处理时间付费。 VM 和容器运行时，即使其上的应用程序处于空闲状态，也会收取相应的费用。 此体系结构并不适用于每个应用 - 但是当应用逻辑可以分离到独立的单元时，可以单独对其进行测试和更新，并在数微秒内启动它们，从而使这种方法成为部署的最快选择。</p><p>以下是比较我们介绍的三种计算方法的关系图。</p><p><img src="/images/2-vm-vs-container-vs-serverless.png" alt="显示虚拟机、容器和无服务器计算的比较的关系图"></p><h3 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h3><p>大多数设备和应用程序都会读取和/或写入数据。 下面是一些示例：</p><ul><li>在线购买电影票</li><li>查询在线商品的价格</li><li>拍摄照片</li><li>发送电子邮件</li><li>留语音邮件</li></ul><p>在以上所有案例中，不是读取数据（查询价格）就是写入数据（拍摄照片）。 在每种情况下，数据类型及其存储方式可能不同。</p><p>云提供商通常会提供可处理所有这些类型的数据的服务。 例如，如果要存储文本或影片剪辑，则可以使用磁盘上的文件。 如果有一组关系（例如地址簿），可以采取一种更结构化的方法，比如使用数据库。</p><p>使用基于云的数据存储的优点是可以缩放以满足需求。 如果发现需要更多空间来存储影片剪辑，则可以支付更多费用并增加可用空间。 在某些情况下，存储甚至可以自动进行扩展和缩放，因此你可以在任何给定的时间点为所需的内容付费。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>每项业务有不同的需求和要求。 云计算灵活且经济高效，不论是小型还是大型企业，均可由此受益。</p>]]></content>
      
      
      <categories>
          
          <category> Cloud computing </category>
          
          <category> Azure </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azure </tag>
            
            <tag> Cloud computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/05/31/cyjournal-8/"/>
      <url>/2020/05/31/cyjournal-8/</url>
      
        <content type="html"><![CDATA[<p>五月的最后一天，生活还是平淡如常。一开始的时候有些厌恶，但是仔细想来，倒也不是一件坏事。无意间发现自己多了一些行为，一天之内居然好几次拿起手机，想法朋友圈，文字配图都被仔细的编辑好了，却又在的时候，退出了编辑，并且选择了不保留。</p><p>无意间翻到了伍佰的歌，好听nice。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200531 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>coursera 4st data management and visualization</title>
      <link href="/2020/05/30/coursera-3/"/>
      <url>/2020/05/30/coursera-3/</url>
      
        <content type="html"><![CDATA[<p>This is my assignment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Sun May 24 11:56:00 2020</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: chenjiahao</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">'dataset.csv'</span>)<span class="comment"># ge the garminder data i need</span></span><br><span class="line"></span><br><span class="line">print(data.shape[<span class="number">0</span>])<span class="comment"># number of observations(rows)</span></span><br><span class="line">print(data.shape[<span class="number">1</span>]) <span class="comment"># number of variables(columns)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data = data.replace(data[<span class="string">'hivrate'</span>].get(<span class="number">0</span>), <span class="number">9999</span>) <span class="comment">#replace missing data</span></span><br><span class="line">    </span><br><span class="line">c1 = data[<span class="string">'country'</span>]</span><br><span class="line">c2 = data[<span class="string">'armedforcesrate'</span>]</span><br><span class="line">c3 = data[<span class="string">'femaleemployrate'</span>]</span><br><span class="line">c4 = data[<span class="string">'employrate'</span>]</span><br><span class="line">c5 = data[<span class="string">'hivrate'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Separate the data I need into series</span></span><br><span class="line"></span><br><span class="line">aw = [c1,c2,c3,c4,c5] <span class="comment"># Convert data to a list</span></span><br><span class="line">result = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>] <span class="comment"># Form an empty list to store the results</span></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">5</span>): <span class="comment"># Generate a statistical table for each column</span></span><br><span class="line">    w = aw[ii] <span class="comment"># Gets the specified column</span></span><br><span class="line">    </span><br><span class="line">    df_show = pd.DataFrame(columns=[<span class="string">'frequency'</span>, <span class="string">'percent'</span>, <span class="string">'cumlativefrequency'</span>,<span class="string">'cumlativepercent'</span>])<span class="comment"># Create an empty dataframe</span></span><br><span class="line">    </span><br><span class="line">    fc1 = pd.value_counts(w)<span class="comment"># Statistical frequency</span></span><br><span class="line">    df_show.index.name = fc1.index.name<span class="comment"># Match coordinate index</span></span><br><span class="line">    df_show[<span class="string">'frequency'</span>] = fc1 <span class="comment"># Data matching</span></span><br><span class="line"></span><br><span class="line">    fc2 = pd.value_counts(w,normalize = <span class="literal">True</span>) <span class="comment">#frequency</span></span><br><span class="line">    df_show[<span class="string">'percent'</span>] = fc2 <span class="comment">#Data matching</span></span><br><span class="line"></span><br><span class="line">    v2 =<span class="number">0</span>;<span class="comment"># Pre allocate memory, speed up</span></span><br><span class="line">    v1 =<span class="number">0</span>;<span class="comment"># Pre allocate memory, speed up</span></span><br><span class="line">    </span><br><span class="line">    df_show= df_show.fillna(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,df_show.shape[<span class="number">0</span>]):<span class="comment"># Processing of accumulated data</span></span><br><span class="line"></span><br><span class="line">        v2 = v2 + df_show.iat[i,<span class="number">0</span>] <span class="comment">#Take out data and accumulate</span></span><br><span class="line">        df_show.iat[i,<span class="number">2</span>] = v2 <span class="comment">#Store data</span></span><br><span class="line"></span><br><span class="line">        df_show.iat[i,<span class="number">1</span>] = df_show.iat[i,<span class="number">1</span>]*<span class="number">100</span> <span class="comment">#Convert data</span></span><br><span class="line">        v1 = v1 + df_show.iat[i,<span class="number">1</span>] <span class="comment">#Take out data and accumulate</span></span><br><span class="line">        df_show.iat[i,<span class="number">3</span>] = v1 <span class="comment"># Store data</span></span><br><span class="line"></span><br><span class="line">    print(df_show) <span class="comment">#show data</span></span><br><span class="line">    result[ii] = df_show <span class="comment"># Store result</span></span><br><span class="line">    </span><br><span class="line">nums =[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>):</span><br><span class="line">    vs1 = result[i]</span><br><span class="line">    nums[i<span class="number">-1</span>] = data.shape[<span class="number">0</span>] - vs1.iat[<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">mark_index = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> data.iat[i,<span class="number">4</span>] == int(<span class="number">9999</span>):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> mark_index:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mark_index.append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> data.iat[i,<span class="number">3</span>] == int(<span class="number">9999</span>):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> mark_index:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mark_index.append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> data.iat[i,<span class="number">2</span>] == int(<span class="number">9999</span>):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> mark_index:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mark_index.append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> data.iat[i,<span class="number">1</span>] == int(<span class="number">9999</span>):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> mark_index:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mark_index.append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">new_datas = data.drop(index = mark_index, axis = <span class="number">0</span> )</span><br><span class="line"></span><br><span class="line">new_datas[<span class="string">'me'</span>] = <span class="string">''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,new_datas.shape[<span class="number">0</span>]):</span><br><span class="line">    new_datas.iat[i,<span class="number">5</span>] = float(new_datas.iat[i,<span class="number">3</span>]) - float(new_datas.iat[i,<span class="number">2</span>]) </span><br><span class="line"></span><br><span class="line">c1 = new_datas[<span class="string">'country'</span>]</span><br><span class="line">c2 = new_datas[<span class="string">'armedforcesrate'</span>]</span><br><span class="line">c3 = new_datas[<span class="string">'femaleemployrate'</span>]</span><br><span class="line">c4 = new_datas[<span class="string">'employrate'</span>]</span><br><span class="line">c5 = new_datas[<span class="string">'hivrate'</span>]</span><br><span class="line">c6 = new_datas[<span class="string">'me'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Separate the data I need into series</span></span><br><span class="line"></span><br><span class="line">aw = [c1,c2,c3,c4,c5,c6] <span class="comment"># Convert data to a list</span></span><br><span class="line">result = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>] <span class="comment"># Form an empty list to store the results</span></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">6</span>): <span class="comment"># Generate a statistical table for each column</span></span><br><span class="line">    w = aw[ii] <span class="comment"># Gets the specified column</span></span><br><span class="line">    </span><br><span class="line">    df_show = pd.DataFrame(columns=[<span class="string">'frequency'</span>, <span class="string">'percent'</span>, <span class="string">'cumlativefrequency'</span>,<span class="string">'cumlativepercent'</span>])<span class="comment"># Create an empty dataframe</span></span><br><span class="line">    </span><br><span class="line">    fc1 = pd.value_counts(w)<span class="comment"># Statistical frequency</span></span><br><span class="line">    df_show.index.name = fc1.index.name<span class="comment"># Match coordinate index</span></span><br><span class="line">    df_show[<span class="string">'frequency'</span>] = fc1 <span class="comment"># Data matching</span></span><br><span class="line"></span><br><span class="line">    fc2 = pd.value_counts(w,normalize = <span class="literal">True</span>) <span class="comment">#frequency</span></span><br><span class="line">    df_show[<span class="string">'percent'</span>] = fc2 <span class="comment">#Data matching</span></span><br><span class="line"></span><br><span class="line">    v2 =<span class="number">0</span>;<span class="comment"># Pre allocate memory, speed up</span></span><br><span class="line">    v1 =<span class="number">0</span>;<span class="comment"># Pre allocate memory, speed up</span></span><br><span class="line">    </span><br><span class="line">    df_show= df_show.fillna(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,df_show.shape[<span class="number">0</span>]):<span class="comment"># Processing of accumulated data</span></span><br><span class="line"></span><br><span class="line">        v2 = v2 + df_show.iat[i,<span class="number">0</span>] <span class="comment">#Take out data and accumulate</span></span><br><span class="line">        df_show.iat[i,<span class="number">2</span>] = v2 <span class="comment">#Store data</span></span><br><span class="line"></span><br><span class="line">        df_show.iat[i,<span class="number">1</span>] = df_show.iat[i,<span class="number">1</span>]*<span class="number">100</span> <span class="comment">#Convert data</span></span><br><span class="line">        v1 = v1 + df_show.iat[i,<span class="number">1</span>] <span class="comment">#Take out data and accumulate</span></span><br><span class="line">        df_show.iat[i,<span class="number">3</span>] = v1 <span class="comment"># Store data</span></span><br><span class="line"></span><br><span class="line">    print(df_show) <span class="comment">#show data</span></span><br><span class="line">    result[ii] = df_show <span class="comment"># Store result</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_for_plt = result[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line">x = data_for_plt.index</span><br><span class="line"></span><br><span class="line">y = data_for_plt[<span class="string">'cumlativefrequency'</span>]</span><br><span class="line">xx = range(<span class="number">0</span>,len(data_for_plt.index)) </span><br><span class="line"></span><br><span class="line">ax1 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.bar(xx,y,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"country"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"cumlativefrequency"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y1 = data_for_plt[<span class="string">'cumlativefrequency'</span>]</span><br><span class="line">ax2 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.bar(xx,y1,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax2.yaxis.tick_right()</span><br><span class="line">ax2.set_xlabel(<span class="string">"country"</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"cumlativefrequency"</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">y2 = data_for_plt[<span class="string">'percent'</span>]</span><br><span class="line">ax3 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">plt.bar(xx,y2,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax3.set_xlabel(<span class="string">"country"</span>)</span><br><span class="line">ax3.set_ylabel(<span class="string">"percent"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y3 = data_for_plt[<span class="string">'frequency'</span>]</span><br><span class="line">ax4 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">plt.bar(xx,y3,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax4.set_xlabel(<span class="string">"country"</span>)</span><br><span class="line">ax4.yaxis.tick_right()</span><br><span class="line">ax4.set_ylabel(<span class="string">"frequency"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()  </span><br><span class="line">plt.savefig(<span class="string">"barChart_country.jpg"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_for_plt = result[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line">x = data_for_plt.index</span><br><span class="line">y = data_for_plt[<span class="string">'cumlativefrequency'</span>]</span><br><span class="line">xx = range(<span class="number">0</span>,len(data_for_plt.index)) </span><br><span class="line"></span><br><span class="line">ax1 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.bar(xx,y,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"armedforcesrate"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"cumlativefrequency"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y1 = data_for_plt[<span class="string">'cumlativefrequency'</span>]</span><br><span class="line">ax2 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.bar(xx,y1,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax2.yaxis.tick_right()</span><br><span class="line">ax2.set_xlabel(<span class="string">"armedforcesrate"</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"cumlativefrequency"</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">y2 = data_for_plt[<span class="string">'percent'</span>]</span><br><span class="line">ax3 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">plt.bar(xx,y2,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax3.set_xlabel(<span class="string">"armedforcesrate"</span>)</span><br><span class="line">ax3.set_ylabel(<span class="string">"percent"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y3 = data_for_plt[<span class="string">'frequency'</span>]</span><br><span class="line">ax4 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">plt.bar(xx,y3,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax4.set_xlabel(<span class="string">"armedforcesrate"</span>)</span><br><span class="line">ax4.yaxis.tick_right()</span><br><span class="line">ax4.set_ylabel(<span class="string">"frequency"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()  </span><br><span class="line">plt.savefig(<span class="string">"barChart_armedforcesrate.jpg"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_for_plt = result[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line">x = data_for_plt.index</span><br><span class="line">y = data_for_plt[<span class="string">'cumlativefrequency'</span>]</span><br><span class="line">xx = range(<span class="number">0</span>,len(data_for_plt.index)) </span><br><span class="line"></span><br><span class="line">ax1 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.bar(xx,y,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"femaleemployrate"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"cumlativefrequency"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y1 = data_for_plt[<span class="string">'cumlativefrequency'</span>]</span><br><span class="line">ax2 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.bar(xx,y1,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax2.yaxis.tick_right()</span><br><span class="line">ax2.set_xlabel(<span class="string">"femaleemployrate"</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"cumlativefrequency"</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">y2 = data_for_plt[<span class="string">'percent'</span>]</span><br><span class="line">ax3 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">plt.bar(xx,y2,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax3.set_xlabel(<span class="string">"femaleemployrate"</span>)</span><br><span class="line">ax3.set_ylabel(<span class="string">"percent"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y3 = data_for_plt[<span class="string">'frequency'</span>]</span><br><span class="line">ax4 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">plt.bar(xx,y3,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax4.set_xlabel(<span class="string">"femaleemployrate"</span>)</span><br><span class="line">ax4.yaxis.tick_right()</span><br><span class="line">ax4.set_ylabel(<span class="string">"frequency"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()  </span><br><span class="line">plt.savefig(<span class="string">"barChart_femaleemployrate.jpg"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_for_plt = result[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line">x = data_for_plt.index</span><br><span class="line">y = data_for_plt[<span class="string">'cumlativefrequency'</span>]</span><br><span class="line">xx = range(<span class="number">0</span>,len(data_for_plt.index)) </span><br><span class="line"></span><br><span class="line">ax1 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.bar(xx,y,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"employrate"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"cumlativefrequency"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y1 = data_for_plt[<span class="string">'cumlativefrequency'</span>]</span><br><span class="line">ax2 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.bar(xx,y1,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax2.yaxis.tick_right()</span><br><span class="line">ax2.set_xlabel(<span class="string">"employrate"</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"cumlativefrequency"</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">y2 = data_for_plt[<span class="string">'percent'</span>]</span><br><span class="line">ax3 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">plt.bar(xx,y2,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax3.set_xlabel(<span class="string">"employrate"</span>)</span><br><span class="line">ax3.set_ylabel(<span class="string">"percent"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y3 = data_for_plt[<span class="string">'frequency'</span>]</span><br><span class="line">ax4 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">plt.bar(xx,y3,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax4.set_xlabel(<span class="string">"employratee"</span>)</span><br><span class="line">ax4.yaxis.tick_right()</span><br><span class="line">ax4.set_ylabel(<span class="string">"frequency"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()  </span><br><span class="line">plt.savefig(<span class="string">"barChart_employrate.jpg"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_for_plt = result[<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line">x = data_for_plt.index</span><br><span class="line">y = data_for_plt[<span class="string">'cumlativefrequency'</span>]</span><br><span class="line">xx = range(<span class="number">0</span>,len(data_for_plt.index)) </span><br><span class="line"></span><br><span class="line">ax1 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.bar(xx,y,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"hivrate"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"cumlativefrequency"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y1 = data_for_plt[<span class="string">'cumlativefrequency'</span>]</span><br><span class="line">ax2 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.bar(xx,y1,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax2.yaxis.tick_right()</span><br><span class="line">ax2.set_xlabel(<span class="string">"hivrate"</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"cumlativefrequency"</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">y2 = data_for_plt[<span class="string">'percent'</span>]</span><br><span class="line">ax3 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">plt.bar(xx,y2,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax3.set_xlabel(<span class="string">"hivrate"</span>)</span><br><span class="line">ax3.set_ylabel(<span class="string">"percent"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y3 = data_for_plt[<span class="string">'frequency'</span>]</span><br><span class="line">ax4 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">plt.bar(xx,y3,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax4.set_xlabel(<span class="string">"hivrate"</span>)</span><br><span class="line">ax4.yaxis.tick_right()</span><br><span class="line">ax4.set_ylabel(<span class="string">"frequency"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()  </span><br><span class="line">plt.savefig(<span class="string">"barChart_hivrate.jpg"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_for_plt = result[<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line">x = data_for_plt.index</span><br><span class="line">y = data_for_plt[<span class="string">'cumlativefrequency'</span>]</span><br><span class="line">xx = range(<span class="number">0</span>,len(data_for_plt.index)) </span><br><span class="line"></span><br><span class="line">ax1 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.bar(xx,y,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"me"</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">"cumlativefrequency"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y1 = data_for_plt[<span class="string">'cumlativefrequency'</span>]</span><br><span class="line">ax2 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.bar(xx,y1,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax2.yaxis.tick_right()</span><br><span class="line">ax2.set_xlabel(<span class="string">"me"</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">"cumlativefrequency"</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">y2 = data_for_plt[<span class="string">'percent'</span>]</span><br><span class="line">ax3 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">plt.bar(xx,y2,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax3.set_xlabel(<span class="string">"me"</span>)</span><br><span class="line">ax3.set_ylabel(<span class="string">"percent"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y3 = data_for_plt[<span class="string">'frequency'</span>]</span><br><span class="line">ax4 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">plt.bar(xx,y3,<span class="number">0.4</span>,color=<span class="string">"green"</span>)</span><br><span class="line">ax4.set_xlabel(<span class="string">"me"</span>)</span><br><span class="line">ax4.yaxis.tick_right()</span><br><span class="line">ax4.set_ylabel(<span class="string">"frequency"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()  </span><br><span class="line">plt.savefig(<span class="string">"barChart_me.jpg"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">print(new_datas.describe(include=<span class="string">'all'</span>))</span><br><span class="line"></span><br><span class="line">dc = new_datas.drop(columns = <span class="string">"country"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,dc.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">5</span>):</span><br><span class="line">        dc.iat[i,ii] = float(dc.iat[i,ii])</span><br><span class="line"></span><br><span class="line">print(dc.describe(include=<span class="string">'all'</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">c2 = dc[<span class="string">'armedforcesrate'</span>]</span><br><span class="line">print(c2.mean())</span><br><span class="line">print(c2.std())</span><br><span class="line">print(c2.quantile(q=<span class="number">0.25</span>))</span><br><span class="line">print(c2.quantile(q=<span class="number">0.50</span>))</span><br><span class="line">print(c2.quantile(q=<span class="number">0.75</span>))</span><br><span class="line"></span><br><span class="line">c3 = dc[<span class="string">'femaleemployrate'</span>]</span><br><span class="line">print(c3.mean())</span><br><span class="line">print(c3.std())</span><br><span class="line">print(c3.quantile(q=<span class="number">0.25</span>))</span><br><span class="line">print(c3.quantile(q=<span class="number">0.50</span>))</span><br><span class="line">print(c3.quantile(q=<span class="number">0.75</span>))</span><br><span class="line"></span><br><span class="line">c4 = dc[<span class="string">'employrate'</span>]</span><br><span class="line">print(c4.mean())</span><br><span class="line">print(c4.std())</span><br><span class="line">print(c4.quantile(q=<span class="number">0.25</span>))</span><br><span class="line">print(c4.quantile(q=<span class="number">0.50</span>))</span><br><span class="line">print(c4.quantile(q=<span class="number">0.75</span>))</span><br><span class="line"></span><br><span class="line">c5 = dc[<span class="string">'hivrate'</span>]</span><br><span class="line">print(c5.mean())</span><br><span class="line">print(c5.std())</span><br><span class="line">print(c5.quantile(q=<span class="number">0.25</span>))</span><br><span class="line">print(c5.quantile(q=<span class="number">0.50</span>))</span><br><span class="line">print(c5.quantile(q=<span class="number">0.75</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">c6 = dc[<span class="string">'me'</span>]</span><br><span class="line">print(c6.mean())</span><br><span class="line">print(c6.std())</span><br><span class="line">print(c6.quantile(q=<span class="number">0.25</span>))</span><br><span class="line">print(c6.quantile(q=<span class="number">0.50</span>))</span><br><span class="line">print(c6.quantile(q=<span class="number">0.75</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line">y = new_datas[<span class="string">'hivrate'</span>]</span><br><span class="line">x1 = new_datas[<span class="string">'me'</span>]</span><br><span class="line">x2 = new_datas[<span class="string">'armedforcesrate'</span>]</span><br><span class="line">x3 = new_datas[<span class="string">'femaleemployrate'</span>]</span><br><span class="line">x4 = new_datas[<span class="string">'employrate'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ax1 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.bar(x1,y,<span class="number">0.4</span>,color=<span class="string">"red"</span>)</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.xticks([])</span><br><span class="line">ax1.set_ylabel(<span class="string">"hivrate"</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">"me"</span>)</span><br><span class="line"></span><br><span class="line">ax2 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">ax2.yaxis.tick_right()</span><br><span class="line">plt.bar(x2,y,<span class="number">0.4</span>,color=<span class="string">"red"</span>)</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.xticks([])</span><br><span class="line">ax2.set_ylabel(<span class="string">"hivrate"</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">"armedforcesrate"</span>)</span><br><span class="line"></span><br><span class="line">ax3 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">plt.bar(x3,y,<span class="number">0.4</span>,color=<span class="string">"red"</span>)</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.xticks([])</span><br><span class="line">ax3.set_ylabel(<span class="string">"hivrate"</span>)</span><br><span class="line">ax3.set_xlabel(<span class="string">"femaleemployrate"</span>)</span><br><span class="line"></span><br><span class="line">ax4 = plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">ax4.yaxis.tick_right()</span><br><span class="line">plt.bar(x4,y,<span class="number">0.4</span>,color=<span class="string">"red"</span>)</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.xticks([])</span><br><span class="line">ax4.set_ylabel(<span class="string">"hivrate"</span>)</span><br><span class="line">ax4.set_xlabel(<span class="string">"employrate"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()  </span><br><span class="line">plt.savefig(<span class="string">"analysis.jpg"</span>)</span><br></pre></td></tr></table></figure><p><strong>console and analysis:</strong></p><p><img src="/images/show_program.png" alt="alt"></p><p>To be specific：</p><p>The following figure is a single variable graph of all data variables.</p><p><img src="/images/country.png" alt="alt"></p><p>For national variables, it is evenly distributed. The rest will not be described in detail.</p><p><img src="/images/arm.png" alt="alt"></p><p><img src="/images/emp.png" alt="alt"></p><p><img src="/images/fam.png" alt="alt"></p><p><img src="/images/hiv.png" alt="alt"></p><p><img src="/images/me.png" alt="alt"></p><p>The following is a graph with HIV rate as the response variable and the independent variable as the other.</p><p><img src="/images/ii.png" alt="alt"></p><p>It can be observed that female employment rate and overall employment rate are positively related to HIV rate. Male employment rate and HIV rate showed a middle high and low side characteristics, but not obvious. The military ratio and HIV rate are high on both sides and low in the middle.</p>]]></content>
      
      
      <categories>
          
          <category> coursera </category>
          
          <category> data management and visualization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> coursera </tag>
            
            <tag> data management and visualization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/05/30/cyjournal-7/"/>
      <url>/2020/05/30/cyjournal-7/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/wsh.jpg" alt="alt"></p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200530 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/05/29/cyjournal-6/"/>
      <url>/2020/05/29/cyjournal-6/</url>
      
        <content type="html"><![CDATA[<p>快毕业了，才突然意识到，我去年给yinan预约了两个写真，还没有拍。真的吐了。这个啥时候才能拍噢。想想还是很激动的。学校的大盘鸡变得不好吃了，以前好好次的，诶，难受。yinan说，要我blog写长一点，那我尽力吧哈哈。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200529 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>奥地利经济学派小论</title>
      <link href="/2020/05/29/eco/"/>
      <url>/2020/05/29/eco/</url>
      
        <content type="html"><![CDATA[<p>和其他学科的历史一样，经济思想史展现为一种不同思想体系的混杂，而这些思想体系曾经分属于秉持各种观念的特定学派。这种把来自不同思想家的观念分类的方法关注特定群体的相似之处，却遮蔽了他们的差异。在十八世纪后半叶声名鹊起的法国重农学派代表了第一个现代经济思想学派。随之而来的是古典经济思想、马克思主义和社会主义。十九世纪末叶，在西欧出现了两个互相冲突的经济思想流派：德国历史学派和奥地利学派。德国历史学派试图通过研究经济史来求取经济真理。在1883年，他们的经验主义方法论成为了早期奥地利学派的靶子。奥地利学派坚持经济学知识源自理论分析而非历史研究。这场方法论论战，或者说对方法的争论，持续了逾二十年。</p><p>推荐一本书：《奥地利经济学入门》</p><p>托马斯泰勒的《奥地利经济学入门》以在1873年到1903年之间任维也纳大学政治经济学教授的卡尔·门格尔（Carl Menger）为开端，尝试阐释奥地利学派的核心观念。1871年，门格尔在他的《国民经济学原理》（德文书名：Grundsätze der Volkswirtschaftslehre，英文译名：Principles of Economics）一书中，提出了一种价值理论，解决了长期困扰那些伟大的古典经济学家的问题。这个理论就是基于边际效用原理的主观价值理论。[注：经济思想史如今已经将几乎在同时分别创立了主观价值理论的荣誉归功于门格尔、英国经济学家威廉·斯坦利·杰文斯（William Stanley Jevons）和法国经济学家列昂·瓦尔拉斯（Leon Walras）。参见马克·布劳格（Mark Blaug），《经济理论回顾》（Economic Theory in Retrospect）（Homewood: Richard D. Irwin, Inc., 1962）第272页~第273页。]它破除了一个经典概念，即事物的价值是一种内在于事物本身的客观衡量。现在，经济财货依据某一使用者期望从对其增量的使用中所得到的满足，而得到主观评价。在本书的后面部分，我们将更全面地讨论整个奥地利学派系统的基石——主观价值论。门格尔的两个伟大门徒弗里德里希·冯·维塞尔（Friedrich von Wieser）和欧根·冯·庞巴维克（Eugen von Böhm-Bawerk）继承了主观价值论，他们深化了它并廓清了其在成本、资本和利息理论领域的全部分支。具体如下：</p><p>维塞尔扩展了门格尔的归属（imputation）问题，解释了资源的价格或成本源自资源所生产出来的消费品的预期价格。这样便揭示了价值的形成乃是一个循环的过程，而门格尔理论所欠缺的成本概念也被补充进了主观价值论。维塞尔的“成本法则”或替代成本学说认为，生产一件产品的成本反映了其他制造商对生产所用资源的竞争出价。成本仅仅是为了将资源从其第二有利可图的用途上吸引过来而必须支付的报酬。</p><p>庞巴维克的伟大贡献是他的资本和利息理论。他强调了在经济过程中时间的重要性并将资本定义为已生产出来的生产要素。他的分析中的关键思想，是采用“迂回”的生产方式能提升人们的生产力。这种生产力的增长既体现在不考虑装备和工具时可生产财货数量的增加，也体现在仅考虑资本财货时的可生产财货上。采用非直接过程导致的等待时期为他对利息这一现象的解释提供了基础。他宣称，在其他条件一样的情况下，人们对现在财货的估值，高于他们对具有相似特点的未来财货的估值。这个假设包含了为售价和成本之间的利润而辩护的基础。这个利润归属于提供中间产品或资本财货所需资金的资本家们。他们的回报是他们的投资被使用的期间所支付的利息，而不是如马克思声称的对工人的剥削。于是主观价值论得以扩展，从而包含了时间偏好法则。虽然奥地利学派的资本理论在某种程度上被修正了，庞巴维克对利息和迂回或称非直接生产过程的实质解释仍然在今天的奥地利学派理论中占据着支配地位。</p><p>路德维希·冯·米塞斯（Ludwig von Mises）和弗里德里希·冯·哈耶克（Friedrich von Hayek）是现代奥地利学派理论家中的两大巨擘。米塞斯在二十世纪二十年代以其对社会主义的质疑而广受其他经济学家关注。他认为，由于缺少市场价格——对他而言这是为了合理配置资源所不可或缺的手段——社会主义在现代经济中完全不可能实现。米塞斯和哈耶克都对奥地利学派理论形成一个整体做出了突出贡献。他们解释了由不受控制的政府信用的扩张如何导致商业周期波动，从而为奥地利学派的框架增添了又一个重要的组成部分。哈耶克关注于“社会中的知识”问题和协调相互作用的市场参与者的行为的不可或缺的需求，为经济学研究提供了至关重要的洞见。本书将大致描述哈耶克、米塞斯，以及米塞斯的两位学生伊斯雷尔·柯兹纳（Israel Kirzner）和穆瑞·罗斯巴德（Murray Rothbard）的观点。这两位都对说明及阐述奥地利学派的分析做出了突出的贡献。</p><p>尽管奥地利学派不再以它对主观价值论的赞同而和其他学派相区别，在奥地利学派的经济分析方法中仍然有一些显著的固有特征使之区别于其他学派。其中一个特征是它严格的方法论立场。关于门格尔在1883年出版的一本书里发起的方法论论战已经列出了参考书。[注：现在已经被译成英文，书名为《经济学与政治学问题》（Problems of Economics and Sociology）(Urbana: University of Illinois Press, 1963).]奥地利学派的经济分析主要植根于理论的、演绎的推理基础之上；经验主义在奥地利学派的理论中只占次要的位置——因此，他们与德国历史学派展开了论战。源自社会环境的经济现象，被奥地利学派视为太过复杂与易变，以至于不被允许使用物理学家所用的实验分析方法。相应地，奥地利学派理论反对把数学作为经济分析工具的方法论立场。概念理解，而非数量关系，被视为经济科学唯一有意义的基础。奥地利学派之父门格尔把坚持和追随定性研究的取向贯穿在他的著作中，就像他的继承者们所做的那样。</p><p>奥地利学派理论中第二个重要的特征是它的方法论个人主义。奥地利学派坚信经济现象并非是对某些社会力量或诸如“社会”这样的具体实体的表达。相反，它们是个人参与经济活动的行动的结果。因此，除非通过分析其基本元素，即个人的行动，总的经济过程就不能得到理解。</p><p>奥地利学派将人性及人类的困境用作分析的材料。受到包括感性知识在内的限制的个人价值标准和人的行动被置于经济科学的中心位置。人类犯错的因素、未来的不确定性、不可避免的时间流逝必须得到应有的关注。这一分析方法穿透了一个发达市场经济表面上的复杂性，并通过考察本质性的市场因素提供了对经济过程的基本理解。一切围绕在经济、市场价格、商业盈亏、利率、通货膨胀以及经济衰退和萧条的神秘感都被驱散了。这些现象不是无法解释的，也不是没有原因的，就如接下来的章节所展示的那样。</p><p>这本书如其书名所意指的那样，呈现了奥地利学派基本理论的概貌。它的重点是自由市场或者说资本主义经济。为了对这里讨论的主题有更深入理解，无疑不能忽视奥地利学派经济学家们富有创造力的著作。必须参考原著，尤其是为了缜密地鉴别现在十分猖獗的政府干预市场过程所造成的严重后果。本书在每章节的结尾处都提供了推荐阅读书目以加深理解。</p><p>实际上，一个人在使用“奥地利学派经济学”一词的时候难免会有些不好意思，因为担心这意味着它可能不同于简明可靠的经济学。凯恩斯主义经济学的混乱，计量经济学的惑人造作，“专业”经济学家糟糕的预测记录，诸如完全竞争和完全垄断等不切实际的教科书模型，持续的通货膨胀和失业，以及经济利益普遍的政治化，已经造成了对所有经济理论确定无疑的不信任。然而，如果要进一步理解市场过程和伴随其运行的干预的效果，就不能忽视奥地利学派的分析。</p><p>关于奥地利学派的另外一些资料已经更新在了资源那篇文章里。</p>]]></content>
      
      
      <categories>
          
          <category> economy </category>
          
          <category> 20200529 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> economy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/05/28/cyjournal-5/"/>
      <url>/2020/05/28/cyjournal-5/</url>
      
        <content type="html"><![CDATA[<p>今天和小猪聊天聊很多，脾气暴躁的我，解锁新称号：暴躁小恐龙。哈哈哈，聊了很多，觉得yinan其实也挺脆弱的。我可能平时太苛刻了吧，对她。应该对她要好一点。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200528 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/05/27/cyjournal-4/"/>
      <url>/2020/05/27/cyjournal-4/</url>
      
        <content type="html"><![CDATA[<p>今日享受生活</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200527 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>coursera 3st data management and visualization</title>
      <link href="/2020/05/27/coursera-2/"/>
      <url>/2020/05/27/coursera-2/</url>
      
        <content type="html"><![CDATA[<p>This is my assignment.</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">'dataset.csv'</span>)<span class="comment"># ge the garminder data i need</span></span><br><span class="line"></span><br><span class="line">print(data.shape[<span class="number">0</span>])<span class="comment"># number of observations(rows)</span></span><br><span class="line">print(data.shape[<span class="number">1</span>]) <span class="comment"># number of variables(columns)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data = data.replace(data[<span class="string">'hivrate'</span>].get(<span class="number">0</span>), <span class="number">9999</span>) <span class="comment">#replace missing data</span></span><br><span class="line">    </span><br><span class="line">c1 = data[<span class="string">'country'</span>]</span><br><span class="line">c2 = data[<span class="string">'armedforcesrate'</span>]</span><br><span class="line">c3 = data[<span class="string">'femaleemployrate'</span>]</span><br><span class="line">c4 = data[<span class="string">'employrate'</span>]</span><br><span class="line">c5 = data[<span class="string">'hivrate'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Separate the data I need into series</span></span><br><span class="line"></span><br><span class="line">aw = [c1,c2,c3,c4,c5] <span class="comment"># Convert data to a list</span></span><br><span class="line">result = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>] <span class="comment"># Form an empty list to store the results</span></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">5</span>): <span class="comment"># Generate a statistical table for each column</span></span><br><span class="line">    w = aw[ii] <span class="comment"># Gets the specified column</span></span><br><span class="line">    </span><br><span class="line">    df_show = pd.DataFrame(columns=[<span class="string">'frequency'</span>, <span class="string">'percent'</span>, <span class="string">'cumlativefrequency'</span>,<span class="string">'cumlativepercent'</span>])<span class="comment"># Create an empty dataframe</span></span><br><span class="line">    </span><br><span class="line">    fc1 = pd.value_counts(w)<span class="comment"># Statistical frequency</span></span><br><span class="line">    df_show.index.name = fc1.index.name<span class="comment"># Match coordinate index</span></span><br><span class="line">    df_show[<span class="string">'frequency'</span>] = fc1 <span class="comment"># Data matching</span></span><br><span class="line"></span><br><span class="line">    fc2 = pd.value_counts(w,normalize = <span class="literal">True</span>) <span class="comment">#frequency</span></span><br><span class="line">    df_show[<span class="string">'percent'</span>] = fc2 <span class="comment">#Data matching</span></span><br><span class="line"></span><br><span class="line">    v2 =<span class="number">0</span>;<span class="comment"># Pre allocate memory, speed up</span></span><br><span class="line">    v1 =<span class="number">0</span>;<span class="comment"># Pre allocate memory, speed up</span></span><br><span class="line">    </span><br><span class="line">    df_show= df_show.fillna(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,df_show.shape[<span class="number">0</span>]):<span class="comment"># Processing of accumulated data</span></span><br><span class="line"></span><br><span class="line">        v2 = v2 + df_show.iat[i,<span class="number">0</span>] <span class="comment">#Take out data and accumulate</span></span><br><span class="line">        df_show.iat[i,<span class="number">2</span>] = v2 <span class="comment">#Store data</span></span><br><span class="line"></span><br><span class="line">        df_show.iat[i,<span class="number">1</span>] = df_show.iat[i,<span class="number">1</span>]*<span class="number">100</span> <span class="comment">#Convert data</span></span><br><span class="line">        v1 = v1 + df_show.iat[i,<span class="number">1</span>] <span class="comment">#Take out data and accumulate</span></span><br><span class="line">        df_show.iat[i,<span class="number">3</span>] = v1 <span class="comment"># Store data</span></span><br><span class="line"></span><br><span class="line">    print(df_show) <span class="comment">#show data</span></span><br><span class="line">    result[ii] = df_show <span class="comment"># Store result</span></span><br><span class="line">    </span><br><span class="line">nums =[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>):</span><br><span class="line">    vs1 = result[i]</span><br><span class="line">    nums[i<span class="number">-1</span>] = data.shape[<span class="number">0</span>] - vs1.iat[<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">mark_index = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> data.iat[i,<span class="number">4</span>] == int(<span class="number">9999</span>):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> mark_index:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mark_index.append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> data.iat[i,<span class="number">3</span>] == int(<span class="number">9999</span>):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> mark_index:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mark_index.append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> data.iat[i,<span class="number">2</span>] == int(<span class="number">9999</span>):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> mark_index:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mark_index.append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,data.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">if</span> data.iat[i,<span class="number">1</span>] == int(<span class="number">9999</span>):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> mark_index:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mark_index.append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">new_datas = data.drop(index = mark_index, axis = <span class="number">0</span> )</span><br><span class="line"></span><br><span class="line">new_datas[<span class="string">'me'</span>] = <span class="string">''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,new_datas.shape[<span class="number">0</span>]):</span><br><span class="line">    new_datas.iat[i,<span class="number">5</span>] = float(new_datas.iat[i,<span class="number">3</span>]) - float(new_datas.iat[i,<span class="number">2</span>]) </span><br><span class="line"></span><br><span class="line">c1 = new_datas[<span class="string">'country'</span>]</span><br><span class="line">c2 = new_datas[<span class="string">'armedforcesrate'</span>]</span><br><span class="line">c3 = new_datas[<span class="string">'femaleemployrate'</span>]</span><br><span class="line">c4 = new_datas[<span class="string">'employrate'</span>]</span><br><span class="line">c5 = new_datas[<span class="string">'hivrate'</span>]</span><br><span class="line">c6 = new_datas[<span class="string">'me'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Separate the data I need into series</span></span><br><span class="line"></span><br><span class="line">aw = [c1,c2,c3,c4,c5,c6] <span class="comment"># Convert data to a list</span></span><br><span class="line">result = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>] <span class="comment"># Form an empty list to store the results</span></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">6</span>): <span class="comment"># Generate a statistical table for each column</span></span><br><span class="line">    w = aw[ii] <span class="comment"># Gets the specified column</span></span><br><span class="line">    </span><br><span class="line">    df_show = pd.DataFrame(columns=[<span class="string">'frequency'</span>, <span class="string">'percent'</span>, <span class="string">'cumlativefrequency'</span>,<span class="string">'cumlativepercent'</span>])<span class="comment"># Create an empty dataframe</span></span><br><span class="line">    </span><br><span class="line">    fc1 = pd.value_counts(w)<span class="comment"># Statistical frequency</span></span><br><span class="line">    df_show.index.name = fc1.index.name<span class="comment"># Match coordinate index</span></span><br><span class="line">    df_show[<span class="string">'frequency'</span>] = fc1 <span class="comment"># Data matching</span></span><br><span class="line"></span><br><span class="line">    fc2 = pd.value_counts(w,normalize = <span class="literal">True</span>) <span class="comment">#frequency</span></span><br><span class="line">    df_show[<span class="string">'percent'</span>] = fc2 <span class="comment">#Data matching</span></span><br><span class="line"></span><br><span class="line">    v2 =<span class="number">0</span>;<span class="comment"># Pre allocate memory, speed up</span></span><br><span class="line">    v1 =<span class="number">0</span>;<span class="comment"># Pre allocate memory, speed up</span></span><br><span class="line">    </span><br><span class="line">    df_show= df_show.fillna(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,df_show.shape[<span class="number">0</span>]):<span class="comment"># Processing of accumulated data</span></span><br><span class="line"></span><br><span class="line">        v2 = v2 + df_show.iat[i,<span class="number">0</span>] <span class="comment">#Take out data and accumulate</span></span><br><span class="line">        df_show.iat[i,<span class="number">2</span>] = v2 <span class="comment">#Store data</span></span><br><span class="line"></span><br><span class="line">        df_show.iat[i,<span class="number">1</span>] = df_show.iat[i,<span class="number">1</span>]*<span class="number">100</span> <span class="comment">#Convert data</span></span><br><span class="line">        v1 = v1 + df_show.iat[i,<span class="number">1</span>] <span class="comment">#Take out data and accumulate</span></span><br><span class="line">        df_show.iat[i,<span class="number">3</span>] = v1 <span class="comment"># Store data</span></span><br><span class="line"></span><br><span class="line">    print(df_show) <span class="comment">#show data</span></span><br><span class="line">    result[ii] = df_show <span class="comment"># Store result</span></span><br></pre></td></tr></table></figure><p><strong>console and analysis:</strong></p><p><img src="/images/con.png" alt="alt"></p><p>To be specific：</p><p>No missing data here. This data mainly shows all countries. It’s evenly distributed</p><p><img src="/images/res1.png" alt="alt"></p><p>The following shows the proportion of the military, which is the same in almost every country. So they are evenly distributed. (I mean the frequency of proportion)</p><p><img src="/images/res2.png" alt="alt"></p><p>Three countries have the same level of female employment (at 39.5%). It also has the highest frequency.</p><p><img src="/images/res3.png" alt="alt"></p><p>Three employment ratios (47.49%, 61.5%, 58.9%) have occurred three times. This means that employment rates are at this level in three countries.</p><p><img src="/images/res4.png" alt="alt"></p><p>For AIDS rates, 25 countries are at the same level (10%), that is, 10% is also the most frequent rate</p><p><img src="/images/res5.png" alt="alt"></p><p>I use the employment rate minus the female employment rate to get the male employment rate. This data can better help to analyze the impact and relationship between the work rates of different genders on the AIDS rate.Among them, the frequency of male employment rate of 7% is the most, which occurs in four countries.</p><p><img src="/images/res6.png" alt="alt"></p><p><strong>Additional notes:</strong></p><ol><li><p>The variable I selected has no classification variables, so I don’t need to subclassify the original data.</p></li><li><p>Second, the original data is a variable of numeric type, so the usual frequency statistics of the table reveal the relationship between the variables is not obvious. Because the numbers of each country are difficult to agree.</p></li><li><p>For the second submission, I have marked the missing data and explained it. Another blog to go to.</p><p>link:<a href="https://augest-chen.github.io/2020/05/24/coursera-1/#more" target="_blank" rel="noopener">https://augest-chen.github.io/2020/05/24/coursera-1/#more</a></p></li><li><p>For the missing data, I use 9999 instead, and then delete the data one by one. 9999 was chosen because this value could not be within the original data range.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> coursera </category>
          
          <category> data management and visualization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> coursera </tag>
            
            <tag> data management and visualization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/05/26/cyjournal-3/"/>
      <url>/2020/05/26/cyjournal-3/</url>
      
        <content type="html"><![CDATA[<p>有的时候，我也不敢相信我和yinan一天微信只发五个消息，也不见一面，即使都在学校。我们这个恋爱居然也就这么谈下去了。真是不可意思</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200526 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IBM WASTON STUDIO 使用指南</title>
      <link href="/2020/05/25/ibm/"/>
      <url>/2020/05/25/ibm/</url>
      
        <content type="html"><![CDATA[<p>IBM Watson 是认知计算系统的杰出代表，也是一个技术平台。认知计算代表一种全新的计算模式，它包含信息分析，自然语言处理和机器学习领域的大量技术创新，能够助力决策者从大量非结构化数据中揭示非凡的洞察。简单来说，Watson能够支持如下方面，包括但不限于：理解自然语言、大数据的理解和分析、动态分析各类假设和问题、精细的个性化分析能力、在相关数据的基础上优化问题解答、在短时间内提炼洞察、发现新的运行模式、在迭代中学习，探索优化的解决方案、云端开发平台，支持生态发展</p><p><strong>上述都是扯淡，到底什么功能用了才知道。</strong></p><p>ibm官方在官网的放了ibm waston studio，兴趣使然想用一下，但是发现guide比较少。干脆自己写一个好了。先放一张官网的截图。</p><p><img src="/images/ij.png" alt="alt"></p><p>初步探索了一下，一个项目（project）基本可以由notebook和spss modeler两个运行的方式，这么说特也许有点不准确，我指的是一个数据源（data asset）可以使用两种方式（不同的code）去进行研究。然后又一个connection的链接，如果本地有mysql这类的数据库，可以直接从数据库中提取数据。实际上waston本身的数据导入的方式也还是比较简单的。</p>]]></content>
      
      
      <categories>
          
          <category> technology </category>
          
          <category> IBM WASTON STUDIO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IBM WASTON STUDIO </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>0.3.....3*3 = ？</title>
      <link href="/2020/05/25/tech-1/"/>
      <url>/2020/05/25/tech-1/</url>
      
        <content type="html"><![CDATA[<p>今天问yinan一个问题：0.3333……3333*3 = ？她告诉我是0.999……99。我差点晕倒。实际上精髓一点讲：这一个极限问题。换得通俗一点，她的数学水平应该是在高等数学以下的。因为本质上：lim0.99999999…=1。这就是说：1和0.9999…表示的是同一个数，只是表示方法的不同。证明如下：</p><p><img src="/images/th.png" alt="alt"></p><p>实际上，对于这个问题，扯点别的。对于这个问题有疑问的人，如果是大学以及以上的学生（接受过高等数学学习），我是觉得有些担忧的。具体如下：</p><p><img src="/images/thr.png" alt="alt"></p>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/05/25/cyjournal-2/"/>
      <url>/2020/05/25/cyjournal-2/</url>
      
        <content type="html"><![CDATA[<p>最近居然开始矛盾起来，又想快点毕业，又想不毕业。严重怀疑是不是一些哲学的视频看多了，导致心思太活跃了点。想的有点多？？by the way，强烈推荐一下李宗盛的《我是真的爱你》，总感觉李宗盛的声音比别的版本要好听那么一点。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200525 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/05/24/cyjournal-1/"/>
      <url>/2020/05/24/cyjournal-1/</url>
      
        <content type="html"><![CDATA[<p>今天和yinan终于见面了啦，2020在学校的第一次见面，着实有点久噢。新操场的微风和视野，的确也有点让人沉迷。yinan说还在在读一次大学，我说那你考本校呀，她说，我想和你一起读。哈哈哈这一句话让我暗自有点小幸福哈哈哈</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200524 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>coursera 2st data management and visualization</title>
      <link href="/2020/05/24/coursera-1/"/>
      <url>/2020/05/24/coursera-1/</url>
      
        <content type="html"><![CDATA[<p>This is my assignment.</p><p><strong>code:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">'dataset.csv'</span>)<span class="comment"># ge the garminder data i need</span></span><br><span class="line"></span><br><span class="line">print(data.shape[<span class="number">0</span>])<span class="comment"># number of observations(rows)</span></span><br><span class="line">print(data.shape[<span class="number">1</span>]) <span class="comment"># number of variables(columns)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">c1 = data[<span class="string">'country'</span>]</span><br><span class="line">c2 = data[<span class="string">'armedforcesrate'</span>]</span><br><span class="line">c3 = data[<span class="string">'femaleemployrate'</span>]</span><br><span class="line">c4 = data[<span class="string">'employrate'</span>]</span><br><span class="line">c5 = data[<span class="string">'hivrate'</span>]</span><br><span class="line"><span class="comment"># Separate the data I need into series</span></span><br><span class="line"></span><br><span class="line">aw = [c1,c2,c3,c4,c5] <span class="comment"># Convert data to a list</span></span><br><span class="line">result = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>] <span class="comment"># Form an empty list to store the results</span></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">5</span>): <span class="comment"># Generate a statistical table for each column</span></span><br><span class="line">    w = aw[ii] <span class="comment"># Gets the specified column</span></span><br><span class="line">    </span><br><span class="line">    df_show = pd.DataFrame(columns=[<span class="string">'frequency'</span>, <span class="string">'percent'</span>, <span class="string">'cumlativefrequency'</span>,<span class="string">'cumlativepercent'</span>])<span class="comment"># Create an empty dataframe</span></span><br><span class="line"></span><br><span class="line">    fc1 = pd.value_counts(w)<span class="comment"># Statistical frequency</span></span><br><span class="line">    df_show.index.name = fc1.index.name<span class="comment"># Match coordinate index</span></span><br><span class="line">    df_show[<span class="string">'frequency'</span>] = fc1 <span class="comment"># Data matching</span></span><br><span class="line"></span><br><span class="line">    fc2 = pd.value_counts(w,normalize = <span class="literal">True</span>) <span class="comment">#frequency</span></span><br><span class="line">    df_show[<span class="string">'percent'</span>] = fc2 <span class="comment">#Data matching</span></span><br><span class="line"></span><br><span class="line">    v2 =<span class="number">0</span>;<span class="comment"># Pre allocate memory, speed up</span></span><br><span class="line">    v1 =<span class="number">0</span>;<span class="comment"># Pre allocate memory, speed up</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,df_show.shape[<span class="number">0</span>]):<span class="comment"># Processing of accumulated data</span></span><br><span class="line"></span><br><span class="line">        v2 = v2 + df_show[<span class="string">'frequency'</span>].get(i) <span class="comment">#Take out data and accumulate</span></span><br><span class="line">        df_show.iat[i,<span class="number">2</span>] = v2 <span class="comment">#Store data</span></span><br><span class="line"></span><br><span class="line">        df_show.iat[i,<span class="number">1</span>] = df_show[<span class="string">'percent'</span>].get(i)*<span class="number">100</span> <span class="comment">#Convert data</span></span><br><span class="line">        v1 = v1 + df_show[<span class="string">'percent'</span>].get(i) <span class="comment">#Take out data and accumulate</span></span><br><span class="line">        df_show.iat[i,<span class="number">3</span>] = v1 <span class="comment"># Store data</span></span><br><span class="line"></span><br><span class="line">    print(df_show) <span class="comment">#show data</span></span><br><span class="line">    result[ii] = df_show <span class="comment"># Store result</span></span><br></pre></td></tr></table></figure><p><strong>console and analysis:</strong></p><p><img src="/images/re.png" alt="alt"></p><p>To be specific：</p><p><img src="/images/a1.png" alt="alt"></p><p>No missing data here. This data mainly shows all countries. It’s evenly distributed</p><p><img src="/images/a2.png" alt="alt"></p><p>Data for 49 countries are missing, accounting for 23% of the total. This means that there are 49 (23%) countries that do not show their share of the military.</p><p><img src="/images/a3.png" alt="alt"></p><p>Data from 35 countries are missing, accounting for 16.43% of the total. This means that 35 (16.43%) countries did not show their female employment rate.</p><p><img src="/images/a4.png" alt="alt"></p><p>Data from 35 countries are missing, accounting for 16.43% of the total. This means that there are 35 (16.43%) countries that do not show their overall employment rate.</p><p><img src="/images/a6.png" alt="alt"></p><p>Data from 66 countries are missing, accounting for 30.98% of the total. This means that there are 35 (30.98%) countries that do not show their HIV rate<br>Considering that my data is not classified data, I don’t do too much analysis on frequency here.</p>]]></content>
      
      
      <categories>
          
          <category> coursera </category>
          
          <category> data management and visualization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> coursera </tag>
            
            <tag> data management and visualization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>负数开三次方根后出现复数问题</title>
      <link href="/2020/05/24/tech/"/>
      <url>/2020/05/24/tech/</url>
      
        <content type="html"><![CDATA[<p>今天在做数值分析的时候，遇到一个有意思的情况。当我在python3.7中使用pow函数，对一个负数计算三次方根后，发现结果带有复数。具体结果如下：</p><p><img src="/images/py.png" alt="alt"></p><p>实际上为了验证是否是源代码出错，我又在mathematica上面做了一遍计算，结果如下：</p><p><img src="/images/mma.png" alt="alt"></p><p>那么根据两个软件的结果对比，应不是程序源码的问题。本质上这是一个数学问题：</p><p>首先，$ (x)^3 = -8 $，这是一个一元三次方根，根据一元n次方程的在复数领域内有n个根（n为正整数）。且复数的n次方根，可以直接得到如下的求解公式：</p><p><img src="/images/ma.png" alt="alt"></p><p>看到这里，我相信大部分就明白了，按道理这个计算是有三个结果的，但是计算机程序默认是选择了一个主值。具体根据计算机程序而已，有的会取（-pi，pi]。当然，对于主值的选择，一般是根据幅角最小来选择的。</p><p>关于如何展示所有结果，用mathematica的话：</p><p>cube root of -8</p><p>她就展示所有的结果。</p>]]></content>
      
      
      <categories>
          
          <category> math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>房产税</title>
      <link href="/2020/05/24/fiance/"/>
      <url>/2020/05/24/fiance/</url>
      
        <content type="html"><![CDATA[<p>房产税是最近热议的一个话题，实际上在中国房产是一个很重要的经济支撑部分。房地产对于中国的经济影响，几乎可以类比股市对于美国的经济。房地产实际上有双重属性。一个是基本需求，社会福利性质，这是居民的刚需。另一个是在信用货币体系下，投资者和投机者一般会采用实物保值。房地产就吸引了大量的投机资本。在中国这种情况更加严重一些，因为国内没有房地产税，从而加剧了这种投机资本的进入。</p><p>这些投资或者投机能够保值么，倒也不见得。理由是房地产的价格也是和产业布局相互连接的。以美国的底特律为例子，美国汽车公司一破产，大量的工人失业，但是房产价格10、20多美元，但是你一买，就需要就缴纳大量的税额。那么既然无法实现保值，或者说保值风险较大，那么就应该对这类的资本进行限制，避免产生更大的风险。</p><p>这也就是个人对于未来中国2020-2025年的中国股市的指数有可以增长30%-50%的预期空间的来源。当资本不在进入房地产或者说从房地产转向其他领域的时候，其他领域的发展一定是能够得到发展。</p><p>再来讨论房产税的可行性，实际上中国地方政府的税收主要是来源于土地增值。如果推进房产税，财政收入会受到影响。同样的户口制度也会给房产税的推进带来冲击。这实际上是财政税收制度不合理导致的。</p><p>考虑中国的房地产泡沫问题，不妨采用类比美国市场上的垃圾债的问题。采用当年四大国有银行上市前，进行的坏账剥离，成立专门的资产管理公司进行管理，应该是可以较为妥善的处理的。</p><p>那么在进行坏账剥离的前提，就是首先需要对于全国的房地产进行统一登记（这一点极其有必要）。具体来说，城市的房地产需要登记所有权、购房资金来源、购房时间、所有人等都需要进行登记，不仅城市的需要登记，农村的也需要进行登记。在这过程中，也可以顺带明确负债买房的比例、打击一下反腐。在这个步骤之后，就根据房产的数量和质量可以确定房产税率了。</p><p>之后还需要确定的是，房产税征收之后的去向，个人认为，房产税的征收之后应该主要流向于公共服务等社会福利的提供。倒不是说税收收入不流向于社会福利的提供，而是个人认为这一税收应该形成类似专项税的效应。对于一些农民工等这类在社会劳动分工中提供了价值的，但是没有购房的群体。不应该征收房产税。反而应该利用房产税税收，来提供福利。</p><p>总结一下，我设想的房产税是，在全国财产管理信息统一登记之后，对于二套房以及以后的房地产进行征收的，并且用以补贴第一套房产的征收机制。</p>]]></content>
      
      
      <categories>
          
          <category> finance </category>
          
          <category> 20200524 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> economy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyjournal</title>
      <link href="/2020/05/23/cyjournal/"/>
      <url>/2020/05/23/cyjournal/</url>
      
        <content type="html"><![CDATA[<p>明儿见要见到我的狗子了，有点小期待噢。今天无题。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200523 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实体经济发展的一些思考</title>
      <link href="/2020/05/22/finance/"/>
      <url>/2020/05/22/finance/</url>
      
        <content type="html"><![CDATA[<p>最近有如下思考：</p><p>首先，我个人观点是金融业击垮实体经济的的一个重要原因在于，市场上不同行业的利润率差异过大导致的。</p><p>当金融行业的利润率远高于市场上其他实体行业的利润率时，市场上的投资者也就不会将投资目标放到其他行业上去了。也就导致了其他行业的衰落。引申开来：国进民退，也是如此。</p><p>其次，作为解决的方案，应该是政府统计的各行业的行业平均利润率，对于利润率高的行业，进行征税，低的进行减税，从而实现资本要素有序流动。</p><p>目前，根据现有网上资料，除了纽约大学有美国的行业平均利润率数据以外，其他国家似乎都是没有这一数据的统计的。所以，个人认为可以进行统计，至于是否有必要进行公开，这个可以后续在进行商讨。但是毫无疑问，这一数据，是对于宏观调控起到很好的作用的。</p><p>另外，眉山剑客陈平的对于经济学的看法个人十分推荐，值得一看。</p>]]></content>
      
      
      <categories>
          
          <category> finance </category>
          
          <category> 20200522 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> economy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyJournal 4</title>
      <link href="/2020/05/22/lovestory-3/"/>
      <url>/2020/05/22/lovestory-3/</url>
      
        <content type="html"><![CDATA[<p>有些时候，也很奇怪。我和yinan可以一整天不说话，各自沉迷在自己的世界里。</p><p>果然，学习才是唯一的好伙伴。对我和yinan都是如此，哈哈。</p><p>话说起来，最近追星的风格都是：男的女性化，女的中性化嘛？？我有一些难以理解，不过既然yinan喜欢，那也应该尊重她嘛。毕竟每个人都有自己的爱好。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200522 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>coursera 1st data management and visualization</title>
      <link href="/2020/05/21/coursera/"/>
      <url>/2020/05/21/coursera/</url>
      
        <content type="html"><![CDATA[<p>After learning the first week of coursera data management and visualization courses, I will provide the first week of related assignments in this blog.</p><p><strong>First week assignnment:</strong></p><p><strong>STEP 1</strong>: Choose a data set</p><p>I chose <strong>GapMinder Codebook</strong> as the data set because I am interested in factors that affect the ratio of AIDS patients in a country.</p><p><strong>STEP 2</strong>: Identify a specific topic of interest</p><p>To be precise, I want to explore the correlation between the employment rate and a country ’s AIDS rate.<br>Specifically, I chose the three variables of <strong>female employment rate, employment rate and armed forces rate</strong> to measure and represent the employment rate of a country. Here I regard the number of troops in disguised form as an employment. The <strong>HIV rate</strong> is represented by HIVrate.</p><p>The reasons why I chose the above variables are as follows: </p><ol><li>AIDS is spread by blood. Therefore, I want to highlight the relationship between the employment rate of different genders and the AIDS rate by comparing the employment rate with that of women. </li><li>Entering the military service is usually not counted in the employed population, so the number of troops should also be considered</li></ol><p><strong>STEP 3:</strong> Prepare a codebook of your own (i.e., print individual pages or copy screen and paste into a new document) from the larger codebook that includes the questions/items/variables that measure your selected topics.)</p><p>According to step 2, I selected the following variables on the codebook, and did a simple treatment of the data set.</p><p><img src="/images/v1.png" alt="alt"></p><p><img src="/images/v2.png" alt="alt"></p><p>According to the codebook, it can be clearly seen that the data source of the armed force rate is work development indicators, and the interpretation of the calculation method of this ratio is armed forces personnel. The specific content is listed on the picture, so I won’t go into details here.</p><p>At the end of this step, I gave my collated variable data set. details as follows:</p><p><img src="/images/v3.png" alt="alt"></p><p><strong>STEP 4</strong>: Identify another specific topic of interest</p><p>The second topic, I want to explore the relationship between the employment rate of different genders and the ratio of AIDS by comparing the employment rate with the employment rate of women.</p><p><strong>STEP 5:</strong> Add questions/items/variables documenting this second topic to your personal codebook.</p><p>The selection of variables and the first topic have not changed, so I wo n’t repeat them here.</p><p><strong>STEP 6:</strong>  literature review</p><p>I focus on studying the following article:</p><p>Under normal circumstances, most people will be confused about my choice of employment rate rather than GDP as a variable, and the correlation between research and AIDS rate. But another author ’s research indicates that Education and employment may therefore become the short-run policy targets in combatting HIV / AIDS prevalence rate among the active population and achieving economic growth. Our results equally suggest that GDP growth by itself may not be an HIV / AIDS reduction driver. Therefore, it seems more reasonable to choose the employment rate.</p><p>The author (Channing Arndt) used the CGE method to study the interaction between the AIDS pandemic and unemployment. In the model, the wages of unskilled and semi-skilled workers are fixed relative to the producer price index. As a result, the level of employment by activity is an equilibrium variable. And to get the conclusion, although it is expected that the pandemic will cause the growth rate of the supply of unskilled and semi-skilled labor to be close to zero, the analysis shows that the AIDS pandemic will also reduce the demand for labor and put the unemployment rate on our “AIDS” basis and fiction Compared with the “No AIDS” scenario, there is basically no change. The pandemic suppressed labor demand through three effects.<br>In fact, this shows that the AIDS rate and the employment rate are highly correlated. However, there are still few research results on the ratio of AIDS to different genders.</p><p>[1] BACHMANN M O, BOOYSEN F L. Health and economic impact of HIV/AIDS on South African households: a cohort study [J]. BMC Public Health, 2003, 3(1): 14.</p><p>[2] AFAWUBO K, MATHEY S. Employment and education effects on HIV/AIDS prevalence rate and economic growth: empirical investigation in ECOWAS [J]. Applied economics letters, 2014, 21(11): 755-9.</p><p><strong>STEP 7:</strong> Based on your literature review, develop a hypothesis about what you believe the association might be between these topics. </p><p>Therefore, based on this, I propose the following topics:<br>Is the employment rate related to the AIDS rate？<br>My assumption is<br>The higher the employment rate, the lower the AIDS rate. They show a negative correlation.<br>The higher the female employment rate, the lower the AIDS rate. They show a negative correlation.<br>The higher the military employment rate, the lower the AIDS rate. They show a negative correlation.</p>]]></content>
      
      
      <categories>
          
          <category> coursera </category>
          
          <category> data management and visualization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> coursera </tag>
            
            <tag> data management and visualization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyJournal 3</title>
      <link href="/2020/05/21/lovestory-2/"/>
      <url>/2020/05/21/lovestory-2/</url>
      
        <content type="html"><![CDATA[<p>今日，啥都没有，注册了几门coursera，看看学习学习。其他的时间在啃MIT的Mathematics for Computer Science的，资料已经上传在resource的那篇文章里。</p><p>说起来似乎有些奇怪，昨日520还是如漆似胶，今天521就是交流几乎为0。当然了天天520那也是甜到发齁了。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200521 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据存放格式与运算速度</title>
      <link href="/2020/05/21/technology-1/"/>
      <url>/2020/05/21/technology-1/</url>
      
        <content type="html"><![CDATA[<p>考虑如下问题，需要在软件中输入一个3*3的矩阵，用以后续的计算，你会采用什么方式体现。</p><p>实际上可能大部分人想到的是：</p><p><img src="/images/juzhen.PNG" alt="alt"></p><p>但是，我们知道一些计算软件的列运算是行运算快很多的，典型的是matlab。</p><p>那么将第一列视为矩阵的行索引，第二列视为列索引，就可以得到以下的结果：</p><p><img src="/images/juzhen2.PNG" alt="alt"></p><p>看起来似乎没有太大的变化，但是当遇到一个规模比较大的矩阵的时候，这样子的做法，其实是可以提高运算速度的。</p><p>此外，倘若延伸到三维数据，或者高维度的数据：</p><p>在第一种方法中，已经较难表示了。</p><p>但是在第二种方法中，只需要增加一列作为新的维度的索引，即可。</p><p>python等基本思路是一样的，第二种方法，基本上都可以在大规模或者高维度的数据处理上，取得较好的计算效率。</p>]]></content>
      
      
      <categories>
          
          <category> technology </category>
          
          <category> data structure </category>
          
      </categories>
      
      
        <tags>
            
            <tag> data structure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K-means在Matlab的应用</title>
      <link href="/2020/05/21/technology/"/>
      <url>/2020/05/21/technology/</url>
      
        <content type="html"><![CDATA[<p>K-means算法是一种简单的聚类算法。算法的目的是使各个样本与所在类均值的误差平方和达到最小</p><p><strong>步骤：</strong></p><p>1.初始化数据。输入表达矩阵作为对象集X，输入指定聚类类数N，并在X中随机选取N个对象作为初始聚类中心。设定迭代中止条件。<br>2.进行迭代。根据相似度准则将数据对象分配到最接近的聚类中心，从而形成一类。初始化隶属度矩阵。<br>3.更新聚类中心。</p><p><strong>K-means面临的问题：</strong></p><p>1.收敛问题</p><p>2.类别数量需要预先给定</p><p><strong>解决方法:</strong></p><p>首先设类别数为1，然后逐步提高类别数，在每一个类别数都用上述方法，一般情况下，总方差会很快下降，直到到达一个拐点；这意味着再增加一个聚类中心不会显著减少方差，保存此时的聚类数。</p><p><strong>Matlab code:</strong></p><p>首先，这里个人使用的包是: Statistics and Machine Learning Toolbox</p><p><img src="/images/Matlab.PNG" alt="alt"></p><p>这个包是官方制作的bug也许少一些，稳定一些。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[Idx,C,sumD,D]=Kmeans(X,K)</span><br><span class="line"><span class="comment">%X: N*P的数据矩阵，N为数据个数，P为单个数据维度</span></span><br><span class="line"><span class="comment">%K: 预先给定类数，为整数</span></span><br><span class="line"><span class="comment">%idx: 聚类结果</span></span><br><span class="line"><span class="comment">%C: 聚类中心位置</span></span><br><span class="line"><span class="comment">%sumD: 类间所有点与该类质心点距离之和</span></span><br><span class="line"><span class="comment">%D: 每个点与所有质心的距离</span></span><br><span class="line">[Idx,C,sumD,D]=Kmeans(X,K,<span class="string">'Param1'</span>,Val1,<span class="string">'Param2'</span>,Val2,…)</span><br><span class="line"><span class="comment">%其中，'Param1'，'Param2'这是参数的意思</span></span><br><span class="line"><span class="comment">%其中val1，val2这是参数数值</span></span><br><span class="line"><span class="comment">%主要的有配置参数：有 ‘Distance’(距离测度),‘Start’（初始质心位置选择方法),‘Replicates’（聚类重复次数）  整数</span></span><br><span class="line"><span class="comment">%‘Distance’数值有：‘sqEuclidean’ 欧式距离（默认时，采用此距离方式）,‘cityblock’ 绝度误差和，又称：L1‘cosine’,针对向量‘correlation’  针对有时序关系的值,‘Hamming’ 只针对二进制数据</span></span><br><span class="line"><span class="comment">%‘Start’数值有：‘sample’ 从X中随机选取K个质心点,‘uniform’ 根据X的分布范围均匀的随机生成K个质心,‘cluster’ 初始聚类阶段随机选择10%的X的子样本（此方法初始使用’sample’方法),matrix 提供一K*P的矩阵，作为初始质心位置集合</span></span><br><span class="line"><span class="comment">%‘Replicates’数值有：整数即可。</span></span><br></pre></td></tr></table></figure><p><strong>案例</strong></p><p>这种方法比较简单，就暂时不放案例了。</p>]]></content>
      
      
      <categories>
          
          <category> technology </category>
          
          <category> matlab kmeans </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matlab </tag>
            
            <tag> Kmeans </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>culture</title>
      <link href="/2020/05/20/culture/"/>
      <url>/2020/05/20/culture/</url>
      
        <content type="html"><![CDATA[<p>基本上欧洲的整个发展历史都是和宗教有关系的。但是中西方的文化差异，有的时候我们并不是能很好的理解这种文化，典型是，看电影的时候，不太看得明白一些大史观的电影。我自己对欧洲的历史还算是比较感兴趣，又看了不少的电影，干脆就打算在这里简单介绍一下圣经中的一些背景文化，有了这些背景文化，至少在观看大部分的电影，应该没啥问题了。当然内容肯定还是比较多的，会分好几期写。</p><p>圣经的第一部分的基本可以这么理解，亚当和夏娃生活在伊甸园中，然后偷吃了善恶果被驱逐出了伊甸园。从此，人类呢，自生自灭了。</p><p>数代之后呢，上帝觉得人类太过于堕落了。因此呢，挑选了一个好人，名字叫做诺亚，让他建造一所大船。就在诺亚造好大船，就带着各种动物7对上船了。（记住这个数字）。一上船，就是12天的大雨，之后呢，诺亚放出了各鸽子，只见鸽子飞回来捡了树枝回来。诺亚就知道水位退下去了。</p><p>大部分人看到这里都知道了这就是诺亚方舟的故事，但是好莱坞的电影《诺亚方舟 创世之旅》就是由以这个故事为基础的。</p><p>同样的，灾难片《2012》其中最后地球是被海水淹没大部分地区的灾难设定也就是源自于此。</p><p>更有意思的是，上面提到鸽子，因为鸽子带着树枝回来，诺亚才知道洪水退去了。因此鸽子在西方背景中被视为和平的象征，那就是和平鸽最早的起源。到这里也就不难猜，这个树枝是什么？当然是橄榄树树枝。</p><p>这一期就到这里吧，下一期讲摩西。</p>]]></content>
      
      
      <categories>
          
          <category> culture </category>
          
          <category> culture20200520 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> culture </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyJournal 2</title>
      <link href="/2020/05/20/lovestory-1/"/>
      <url>/2020/05/20/lovestory-1/</url>
      
        <content type="html"><![CDATA[<p>记录感情生活的第二天，今天2020.05.20，和yinan一起出去享受生活啦。</p><p>虽然yinan在前一天，叮咛了几百遍，让我别迟到，然后自己今天迟到了。让我一个人傻乎乎的在地铁站等她。有点生气的噢！</p><p>中午，去了食不食货，这家店还是挺不错的。尤其是这个是冰镇小龙虾，似乎是放了青红酒吧，也许是啤酒，觉得味道还可以，虾的个头也比较大。满足～</p><p><img src="/images/shibushihuo.JPG" alt="alt"></p><p>可惜了龙骨感觉就一般了，我以为是烘箱烘的肉，结果却是腌制的，味道也一般吧。皮皮虾的个头倒是甚合我心。捞汁毛豆味道也比较有特色，具体形容起来就是，辣椒油混合了醋，再加点糖？感觉挺喜欢的这个口味的。</p><p>下午就去撸猫啦，yinan还忽悠我我，说是去鬼屋哈哈哈。以为我胆小吗？？</p><p><img src="/images/maoshe1.JPG" alt="alt"></p><p><img src="/images/maoshe2.JPG" alt="alt"></p><p>可惜，下午可能去的时候猫舍，猫猫们似乎都在晒太阳睡觉。不过还是挺可爱的。</p><p>最后是花鸟市场啦，展示一下下午的收获。</p><p>买给yinan的花儿</p><p><img src="/images/hua1.JPG" alt="alt"></p><p>我的最爱郁金香</p><p><img src="/images/hua2.jpg" alt="alt"></p><p>超大束的进口满天星</p><p><img src="/images/hua3.jpg" alt="alt"></p><p>写到最后突然意识到，今天大概是在一起三年了。</p><p>也算得上是一人两花三年了</p><p><img src="/images/end.JPG" alt="alt"></p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200520 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cyJournal 1</title>
      <link href="/2020/05/19/lovestory/"/>
      <url>/2020/05/19/lovestory/</url>
      
        <content type="html"><![CDATA[<p>尝试记录一下自己的感情生活，也算是记录自己的生活吧。这是第一篇关于自己的情感生活的blog。实际上知道我的人都知道，我不怎么喜欢将一些情感生活记录下来。但是还是想突破的尝试一下。现在是2020.5.19.19:00，大部分情侣还是和往常一样在准备着如何过5.20。我和yinan也不例外。</p><p>实际上，今天我委婉的向她表达了，我觉得一年中不需要过这么多的纪念日，她的对与纪念日的要求可以说是很细致了，从早到晚都安排的满满的，每一次都能给我很多的惊喜。也不知明天她给我的惊喜是什么？</p><p>但是，为什么我还是今天委婉的表达了，可以不需要过这么多纪念日呢，其实准确的说，我是觉得绝大多数的纪念日可以通过出去吃个饭，去买点喜欢的，来庆祝。而，只有少部分的生日、纪念日等需要相对隆重的仪式感。我只是觉得每个小日子都过的很有仪式感，会显得很累。但是我想说明的是，过的有仪式感和过的累，并不是划等号的。更加准确的说，我觉得仪式感可以换更加多样化的方式获得，而不是单一化的过节日来获得。（我知道yinan是一个非常看重仪式感的人）。</p><p>她今天可能不是很能理解我的言语，似乎有些生气了。希望她能理解吧。</p>]]></content>
      
      
      <categories>
          
          <category> storys </category>
          
          <category> 20200519 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyJournal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>financial engineering 2</title>
      <link href="/2020/05/19/financial-engineering-1/"/>
      <url>/2020/05/19/financial-engineering-1/</url>
      
        <content type="html"><![CDATA[<p><strong>算法交易和T0交易的区别</strong></p><p>在其他地方看一个博主的回答个人觉得比较符合实际（2020.5.19日以前的情况），在引用完全文后，还有个人的一些见解。</p><p>T0和算法交易从模型层面上来讲，区别并不是很大。都是基于股票横截面和时序数据做短周期预测，只是由于T0需要承担手续费，而算法交易则需要每天把交易需求全部做完，需要更多的交易信号以保证这一点，所以从信号强度来讲T0的信号强度会比算法交易大很多，同时信号个数会比算法交易少很多。不过算法比T0复杂的地方在于挂撤单和交易进度的控制，很多时候如果持续没有合适的信号或者时间比较紧张，挂单又不能成交，只能去敲一些对手价，这个时候就会产生亏损，如何控制交易进度并减少敲对价产生的损失，这又是另外一个问题了。</p><p>回到这个问题，那个方向更有前途。从目前业内状况看来，在任何一家alpha靠谱且管理规模大于2个亿的私募，算法交易都是比T0有前途的多的方向，整个方向的利润空间，生存压力都不是程序化T0所能想象的。这一点可能跟外界了解的区别甚大，这其中缘由，恐怕是18年各个券商私募的一轮造神运动中把T0吹上了天，而外界往往又不了解算法交易的利润空间所致，毕竟各位大佬们出去吹牛，可以吹我们程序化T0年化30%，可是没法吹算法交易年化30%，因为这理论上就不可能。</p><p>从截止本文的现实来看，算法交易至少在以下几个方面是远胜于T0的</p><p><strong>一、利润空间</strong></p><p>以一家靠谱且规模较小的私募为例，假设其总的股票持仓为2亿，每天的换手率25%，一个靠谱的算法交易员，其单边收益应该在万三到万六之间，这里取万4，双边就是万八，那么每天的底仓收益率大概是万二左右，年化是5%，大概对应业绩是1000万。5%在T0行业是什么概念呢，17年人工T0比较厉害的团队7%左右，程序化能做到3-4%就不错了。18年比较厉害的人工交易员团队能做到3-5%，程序化3%都有困难，19年除开上半年比较疯狂的几个月，下半年人工都做不到每天万一，程序化收益能做到年化1.5%已经不算差，也就是说，除了大牛市小牛市，程序化T0对底仓收益的贡献都是远低于算法交易的；</p><p><strong>二、资金容量</strong></p><p>前面说到算法交易信号强度要求低，触发次数多，而且有很多单纯的挂单成交，而T0由于要计算手续费，必然会要求比较强的信号，因此触发次数少很多，这就会带来另外一个问题，资金容量。比如一个T0的信号，可能要求强到有千3左右的费后预期收益才会去做，因为考虑到手续费和冲击成本，抢单难度，低于这个预期收益的信号实盘中可能会亏钱，而算法交易则不同，由于不需要手续费，可以做比较弱的信号，而此时的抢单难度不高，信号比较多，一次冲击成本也不会太大，因此可能预期有一个千二左右费前收益的信号就会做，考虑到千1.4左右的手续费，实际上这两个信号的强度差距千2.4，这样带来的信号个数可能是5-10倍的差距，相应的资金容量也会有类似的差距，也就是，同样一套模型，在T0上可能是3-5亿资金容量，在算法交易上就会有15-50亿的资金容量，这两者的差距显然是没有任何办法可以弥补的；</p><p><strong>三、竞争压力</strong></p><p>第一、目前国内程序化T0从产生到现在，一直面对一个强大的竞争对手，人工T0。目前业内前几名的私募，比如九坤幻方，都是有成规模的人工T0团队的，也就意味着这些公司的程序化T0在这么几年的发展中，依然比不过人工交易员，也不见业内有其他团队跳出来声称自己比人工T0强，目前市场上几家接程序化T0券的公司靠的是比人工交易员低一大截的提成，以及部分公司无力组建人工交易员团队，同时给出的提成又无法满足独立运营人工交易员团队的需求；第二，从2018年以来，很多期货高频团队受外资竞争影响，原有业务盈利下降非常厉害，甚至到了生存不下去的地步，因此逐步转型股票T0。实际上，这个市场的资金容量和利润空间都是极其有限的，但是由于私募行业整体上的封闭性和18年以来各大券商私募的错误宣传，导致这些人认为程序化T0是一个人傻钱多的行业，实际上到目前为止依然有期货高频团队排队入场，后果是这些人并没有开辟出新的方法来赚钱，反而是在原有的策略上大家相互抢钱，这也解释了为何19下半年在行情并没有明显比17年差的情况下，T0收益下降这么多。  </p><p>作者：匿名用户<br>链接：<a href="https://www.zhihu.com/question/354264347/answer/883277309" target="_blank" rel="noopener">https://www.zhihu.com/question/354264347/answer/883277309</a><br>来源：知乎</p><p><strong>个人见解</strong></p><p>首先，程序化交易无论是从掩盖交易量，还是提升利润空间都是可行的。资金量大的基金，更加希望能够借助程序化交易掩盖自身下单行为造成的市场影响，小规模的基因，则更加提升利润空间。</p><p>但是程序化交易的市场空间更大，T0的交易空间相对较小。在随着外资的引入，市场体系不断趋向于成熟，市场能出现适合做T0的机会注定是越来越少的。从长远看，这应该是符合有效市场理论的（这就不做过多展开了）。</p><p>其次，那么来讨论一下市场越来越成熟，难道程序化交易不受到影响么，受到！但是，问题是日内的T0，基本上要千5的利润空间才会去做，但是，程序化即使千1的利润空间也能去做。</p><p>不妨考虑，一个股票，已知今天收盘前需要平仓，T0由于摩擦、交易成本等，需要千5的利润空间才会去做，因为我要卖出，买入，再卖出，但是程序交易，我只需要千1的空间，因为我只要找机会卖出即可。</p><p>那么随着市场的完善和成熟，高利润点，注定是先受到影响的。所以成熟市场，程序化交易受到的影响比T0要小。</p><p>最后，凡是能做T0的时间点，程序化交易也可以做。但是能做程序化的，T0不一定能做。具体参考上面的例子。</p><p>如果你有任何的见解，请与我联系：<strong><a href="mailto:qianniao0825@icloud.com">qianniao0825@icloud.com</a></strong></p>]]></content>
      
      
      <categories>
          
          <category> financial engineering </category>
          
          <category> T0 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> financial engineering </tag>
            
            <tag> T0 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>financial engineering</title>
      <link href="/2020/05/18/financial-engineering/"/>
      <url>/2020/05/18/financial-engineering/</url>
      
        <content type="html"><![CDATA[<p><strong>This is my first financial engineering-themed blog. Here I want to record a summary of my undergraduate thesis. The specific content can be obtained in the download link below.</strong></p><p>Financial risk is a concept that financial markets cannot ignore. After the 2008 financial crisis, research in this field reached its peak. In recent years, with the trend of trade protectionism and globalization, the market attaches great importance to financial risks. Therefore, it is necessary to carry out risk assessment of financial markets. The stock market is a barometer of the economy, so this paper chooses to make a risk assessment for the index of the stock market. First of all, the index (CSI 300) of the reciprocal income, as a label. Then, 523 sets of factors were extracted and designed, and after the factor effectiveness analysis, the distribution pattern was selected after the collinear processing. The 294 factors obtained were normalized and finally predicted by the LSTM model. At the same time, the prediction results are optimized, and the results after optimization are compared with the prediction results of undeleted, the prediction results of non-log labels, and the prediction results of multi-nonlinear regression, and the evaluation functions designed by themselves are used for analysis.  </p><p>The conclusion shows that in the financial market, logarithm label is more suitable for prediction than non-log label, the optimized LSTM prediction performance is better than multi-nonlinear regression, the evaluation function of design has a good evaluation of the forecast demand, and the necessity of collinear processing of factor data is demonstrated. The treatment of normalized, in this paper is more suitable for the prediction model.  </p><p>You can contact me via the following email: <a href="mailto:qianniao0825@icloud.com">qianniao0825@icloud.com</a></p><p><strong>Full text link</strong> <a href="/download/benke.pdf">download</a></p>]]></content>
      
      
      <categories>
          
          <category> financial engineering </category>
          
          <category> financial risk </category>
          
      </categories>
      
      
        <tags>
            
            <tag> financial engineering </tag>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>“Resources”</title>
      <link href="/2020/05/18/%E2%80%9CResources%E2%80%9D/"/>
      <url>/2020/05/18/%E2%80%9CResources%E2%80%9D/</url>
      
        <content type="html"><![CDATA[<p>Here, I will list some books on using and learning (some of them are written by myself). Hope it helps you.   </p><p><strong>Important:</strong> All books listed here are for learning use only. If you feel that some copyrights have been infringed, please contact me. You can contact me via the following email: <a href="mailto:qianniao0825@icloud.com">qianniao0825@icloud.com</a></p><p><strong>1.  SAS guide 9.4</strong> <a href="/download/SAS9.4.pdf">download</a></p><p><strong>2. Mathematics for Computer Science（MIT2015）</strong><a href="/download/Mcs.pdf">download</a></p>]]></content>
      
      
      <categories>
          
          <category> free download </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Resources </tag>
            
            <tag> download </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tagscloud</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>about me</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<p>Hello, this is augest-chen.<br>I am a researcher engaged in quantitative finance and modern statistics.<br>You can contact me via the following email: <a href="mailto:qianniao0825@icloud.com">qianniao0825@icloud.com</a><br>You are welcome to exchange any content with me about financial markets and modern statistics.<br>I also use some software: Python, Matlab, R, Mathematica, SAS. I will also write some summaries of my personal experience, which will be put in the blog, I hope these summaries will play a role for you.</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/baidu_verify_mt1uVtI4Qf.html"/>
      <url>/baidu_verify_mt1uVtI4Qf.html</url>
      
        <content type="html"><![CDATA[mt1uVtI4Qf]]></content>
      
    </entry>
    
    
  
</search>
